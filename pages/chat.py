# chat.py
import streamlit as st
from datetime import datetime

from langchain_community.vectorstores import Chroma
from langchain.embeddings import HuggingFaceEmbeddings

from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.prompts import ChatPromptTemplate

from langchain_teddynote import logging
from dotenv import load_dotenv
import os

# ì±—ë´‡ ì´ë¯¸ì§€ ë§í¬ ì„ ì–¸
botImgPath = 'https://raw.githubusercontent.com/kbr1218/streamlitTest/main/imgs/dolhareubang3.png'

# í˜ì´ì§€ ì œëª© ì„¤ì •
st.set_page_config(page_title="chat", page_icon="ğŸ’¬", layout="wide",
                   initial_sidebar_state='expanded')

from pages.subpages import sidebar, chat_search

# ì‚¬ì´ë“œë°”
with st.sidebar:
    sidebar.show_sidebar()


##########################
### 00. í™˜ê²½ë³€ìˆ˜ ë¡œë“œ ###
load_dotenv()
GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')
# langsmith ì¶”ì  ì„¤ì •
logging.langsmith("bigcon_langchain_test")


### 1. HuggingFace ì„ë² ë”© ìƒì„± ###
embeddings  = HuggingFaceEmbeddings(model_name="jhgan/ko-sroberta-multitask")


## 2. Chroma ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ###
vectorstore = Chroma(persist_directory="./sample_1000_vectorstore", embedding_function=embeddings)
temperature_vectorstore = Chroma(persist_directory="./temperature_vectorstore", embedding_function=embeddings)


## 3. ì‚¬ìš©ì ì •ë³´ ê¸°ë°˜ ì§€ì—­ í•„í„°ë§ ###
user_name = st.session_state.get('user_name', None)
user_age = st.session_state.get('age', None)
visit_times = st.session_state.get('visit_times', None)
visit_region = st.session_state.get('region', [])
visit_dates = st.session_state.get('visit_dates', None)
# ì›” ì •ë³´ë§Œ ì¶œë ¥
visit_month = f"{visit_dates.month}ì›”" if visit_dates else ""

### 3-1. ì‚¬ìš©ì ë°ì´í„°ì™€ ì¼ì¹˜í•˜ëŠ” ì»¬ëŸ¼ëª… í…ìŠ¤íŠ¸ ìƒì„± ###
age_col = f'{user_age} íšŒì›ìˆ˜ ë¹„ì¤‘'

weekday_idx = visit_dates.weekday()
weekdays = ['ì›”ìš”ì¼', 'í™”ìš”ì¼', 'ìˆ˜ìš”ì¼', 'ëª©ìš”ì¼', 'ê¸ˆìš”ì¼', 'í† ìš”ì¼', 'ì¼ìš”ì¼']
weekday_col = f'{weekdays[weekday_idx]} ì´ìš©ê±´ìˆ˜ ë¹„ì¤‘'

time_col = {
    "ì•„ì¹¨ (05-11ì‹œ)": "5ì‹œ-11ì‹œ ì´ìš©ê±´ìˆ˜ ë¹„ì¤‘",
    "ì ì‹¬ (12-13ì‹œ)": "12ì‹œ-13ì‹œ ì´ìš©ê±´ìˆ˜ ë¹„ì¤‘",
    "ì˜¤í›„ (14-17ì‹œ)": "14ì‹œ-17ì‹œ ì´ìš©ê±´ìˆ˜ ë¹„ì¤‘",
    "ì €ë… (18-22ì‹œ)": "18ì‹œ-22ì‹œ ì´ìš©ê±´ìˆ˜ ë¹„ì¤‘",
    "ì‹¬ì•¼ (23-04ì‹œ)": "23ì‹œ-4ì‹œ ì´ìš©ê±´ìˆ˜ ë¹„ì¤‘"
    }.get(visit_times)

### 4. ê¸°ì˜¨ ë°ì´í„° ë¡œë“œ ###
temp_retriever = temperature_vectorstore.as_retriever(
    search_type="mmr",   
    search_kwargs={"k": 5}  # ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ í•œ ê°œì˜ ë¬¸ì„œë§Œ ê°€ì ¸ì˜¤ê¸°
)

### 5. ê²€ìƒ‰ê¸° ìƒì„± ###
retriever = vectorstore.as_retriever(
    search_type="mmr",   
    search_kwargs={"k": 10,              # ë°˜í™˜í•  ë¬¸ì„œ ìˆ˜ (default: 4)
                   "fetch_k": 50,        # MMR ì•Œê³ ë¦¬ì¦˜ì— ì „ë‹¬í•  ë¬¸ì„œ ìˆ˜
                   "lambda_mult": 0.5,    # ê²°ê³¼ ë‹¤ì–‘ì„± ì¡°ì ˆ (default: 0.5),
                   'filter': {'ì§€ì—­': {'$in':visit_region}}
                   }
    # filters={"ì§€ì—­":visit_region}
)

# Do not include unnecessary information. 
### 6. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„¤ì • ###
template = """
You are an assistant named 'ì¹œì ˆí•œ ì œì£¼Â°C' specializing in recommending restaurants in Jeju Island based on specific data.
Use the provided data to answer accurately. If unsure, respond that you don't know.

- When the user's question is a general recommendation request, follow the structured format below.
- If the user's question is asking for a specific piece of information (e.g., "What is the local visitation rate for ê³µëª…ì‹ë‹¹?"), provide only the requested information without additional formatting or explanation.
- Always base your answer strictly on the given data context.

When making recommendations, consider the user's visiting day, age group, and preferred time slot, recommending 1-3 places at most.
You have to start with a summary of the relevant temperature information from the retrieved context for {visit_month} and {visit_times}, and then continue with restaurant recommendations.

The following columns are relevant for finding the best recommendations:
- Weekday column: {{day_column}}
- Time slot column: {{time_column}}
- Age group column: {{age_column}}

**Structured Format** (for general recommendations):
"**{{user_name}}**ë‹˜! {{visit_month}}ì›” {{visit_times}}ì— {{visit_region}} ì§€ì—­ì—ì„œ ì¸ê¸° ìˆëŠ” ë§›ì§‘ì„ ì¶”ì²œë“œë¦¬ê² ìŠµë‹ˆë‹¤! \n
{{visit_month}}ì›” {{visit_times}}ì˜ {{visit_region}}ì˜ í‰ê·  ê¸°ì˜¨ì€ {{average_temperature}}ì…ë‹ˆë‹¤. ì—¬í–‰ì— ì°¸ê³ í•˜ì‹œê¸¸ ë°”ëë‹ˆë‹¤. \n

**{{ê°€ë§¹ì ëª…}}**:
- ì£¼ì†Œ: {{ì£¼ì†Œ}}
- {{visit_month}}ì›” {{visit_region}} ì§€ì—­ì—ì„œ {{user_age}}ì˜ ë°©ë¬¸ ë¹„ìœ¨ì´ {{age_col}}%ë¡œ {{user_name}}ë‹˜ê³¼ ë¹„ìŠ·í•œ ì—°ë ¹ëŒ€ì˜ ê³ ê°ì´ ë§ì´ ì°¾ì•˜ìŠµë‹ˆë‹¤.
- {{user_name}}ë‹˜ì´ ë°©ë¬¸í•˜ì‹œë ¤ëŠ” {{weekdays[weekday_idx]}}ì—ëŠ” ë°©ë¬¸ ë¹„ì¤‘ì´ {{weekday_col}}%ì…ë‹ˆë‹¤.
- {{visit_times}}ì˜ ì´ìš© ê±´ìˆ˜ ë¹„ì¤‘ì€ {{time_col}}% ìœ¼ë¡œ ë†’ì€/ë‚®ì€ í¸ì…ë‹ˆë‹¤.
- ì´ ë§›ì§‘ì˜ ì›”ë³„ ì—…ì¢…ë³„ ì´ìš©ê±´ìˆ˜ ë¶„ìœ„ìˆ˜ êµ¬ê°„ì€ {{ì›”ë³„ ì—…ì¢…ë³„ ì´ìš©ê±´ìˆ˜ ë¹„ì¤‘}}ì— ì†í•˜ë©°, ì›”ë³„ ì—…ì¢…ë³„ ì´ìš©ê¸ˆì•¡ ë¶„ìœ„ìˆ˜ êµ¬ê°„ëŠ” {{ì›”ë³„ ì—…ì¢…ë³„ ì´ìš©ê¸ˆì•¡ ë¶„ìœ„ìˆ˜ êµ¬ê°„}}ì…ë‹ˆë‹¤. ë°©ë¬¸í•˜ì‹œê¸° ì „ì— ì°¸ê³ í•˜ì„¸ìš”!
- ì£¼ë³€ ê´€ê´‘ì§€: ë§›ì§‘ê³¼ ê°€ê¹Œìš´ ê³³ì— **{{ë§›ì§‘ ì£¼ë³€ ê´€ê´‘ì§€}}**ì´(ê°€) ìˆìŠµë‹ˆë‹¤.

ì¦ê±°ìš´ ì‹ì‚¬ ë˜ì‹œê¸¸ ë°”ëë‹ˆë‹¤!"

**When providing specific details for questions like:** "What is the local visitation rate for ê³µëª…ì‹ë‹¹?" Answer only the specific information, in simple and polite format with specific rate.

Use the provided context and user information strictly:
[context]: {context}
---
[ì§ˆì˜]: {query}
"""
prompt = ChatPromptTemplate.from_template(template)


### 7. Google Gemini ëª¨ë¸ ìƒì„± ###
# @st.cache_resource
def load_model():
    system_instruction = """ë‹¹ì‹ ì€ ì œì£¼ë„ ì—¬í–‰ê°ì—ê²Œ ë§›ì§‘ì„ ì¶”ì²œí•˜ëŠ” 'ì¹œì ˆí•œ ì œì£¼Â°C' ì±—ë´‡ì…ë‹ˆë‹¤. 
        ê° ëŒ€í™”ì—ì„œ í•„ìš”í•œ ì •ë³´ë¥¼ ì •í™•íˆ ì œê³µí•˜ê³ , ì‚¬ìš©ìì˜ ì§ˆë¬¸ì´ í›„ì† ì§ˆë¬¸ì¸ ê²½ìš° ì´ì „ ëŒ€í™”ì˜ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.
        í•„ìš”í•œ ê²½ìš° ê°„ê²°í•˜ê²Œ ì •ë³´ë¥¼ ì œê³µí•˜ê³ , ëŒ€í™”ì˜ ë§¥ë½ì„ ìœ ì§€í•˜ì—¬ ì§ˆë¬¸ê³¼ ê´€ê³„ ì—†ëŠ” ì •ë³´ë¥¼ ìƒëµí•˜ì„¸ìš”.
        """
    model = ChatGoogleGenerativeAI(
        model="gemini-1.5-flash",
        temperature=0.3,
        max_tokens=5000,
        system_instruction=system_instruction
    )
    print("model loaded...")
    return model


### 8. ê²€ìƒ‰ ê²°ê³¼ í•„í„°ë§ & ë³‘í•© í•¨ìˆ˜ ###
# visit_region ë°ì´í„° í•„í„°ë§
def filter_results_by_region(docs, visit_region):
    return [doc for doc in docs if doc.metadata.get('ì§€ì—­') in visit_region]

def format_docs(docs):
  return "\n\n".join(doc.page_content for doc in docs)

def retrieve_and_filter_context(_input):
    # temp_retrieverì™€ retriever ê°ê° í˜¸ì¶œ ë° í•„í„°ë§ í›„ ë³‘í•©
    temp_docs = filter_results_by_region(temp_retriever.invoke(_input), visit_region)
    main_docs = filter_results_by_region(retriever.invoke(_input), visit_region)

    # í•„í„°ë§ëœ ê²°ê³¼ê°€ ì—†ë‹¤ë©´ ì˜¤ë¥˜ ë©”ì‹œì§€ ë°˜í™˜
    if not temp_docs and not main_docs:
        return "ë§ì”€í•˜ì‹  ì§€ì—­ì— ëŒ€í•œ ë§›ì§‘ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‚¬ì´ë“œë°”ì—ì„œ ë°©ë¬¸í•˜ì‹¤ ì§€ì—­ì„ ë‹¤ì‹œ ì„ íƒí•´ì£¼ì„¸ìš”."
    # ë³‘í•© í›„ í˜•ì‹í™”
    return format_docs(temp_docs + main_docs)

## 9. LangChain ì²´ì¸ êµ¬ì„± ###
rag_chain = (
  {"query":RunnablePassthrough(),
    "context": retrieve_and_filter_context,
    "user_name":RunnablePassthrough(),
    "user_age":RunnablePassthrough(),
    "visit_times":RunnablePassthrough(),
    "visit_month":RunnablePassthrough(),
    "visit_region":RunnablePassthrough()
  }
  # question(ì‚¬ìš©ìì˜ ì§ˆë¬¸) ê¸°ë°˜ìœ¼ë¡œ ì—°ê´€ì„±ì´ ë†’ì€ ë¬¸ì„œ retriever ìˆ˜í–‰ >> format_docsë¡œ ë¬¸ì„œë¥¼ í•˜ë‚˜ë¡œ ë§Œë“¦
  | prompt               # í•˜ë‚˜ë¡œ ë§Œë“  ë¬¸ì„œë¥¼ promptì— ë„˜ê²¨ì£¼ê³ 
  | load_model()         # llmì´ ì›í•˜ëŠ” ë‹µë³€ì„ ë§Œë“¦
  | StrOutputParser()
)


### 10. Streamlit UI ###
st.subheader("ğŸŠ:orange[ì œì£¼Â°C]ì—ê²Œ ì§ˆë¬¸í•˜ê¸°")
st.divider()

say_hi_to_user = """ì•ˆë…•í•˜ì„¸ìš”!  
ì œì£¼ë„ì˜ ì§€ì—­/ì‹œê°„ë³„ ê¸°ì˜¨ ë°ì´í„°ì— ê¸°ë°˜í•˜ì—¬ ë§›ì§‘ì„ ì¶”ì²œí•˜ëŠ” :orange[**ì¹œì ˆí•œ ì œì£¼Â°C**]ì…ë‹ˆë‹¤.  
ì–¸ì œë“ ì§€ ì§ˆë¬¸í•´ì£¼ì„¸ìš”."""

user_input = st.chat_input(
    placeholder="ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”. (ì˜ˆ: ì¶”ìë„ì— ìˆëŠ” ê°€ì •ì‹ ë§›ì§‘ì„ ë‘ ê°œë§Œ ì¶”ì²œí•´ì¤˜)",
    max_chars=150
)

chat_col1, search_col2 = st.columns([2, 1])
with search_col2:
    chat_search.show_search_restaurant()

    # ì±„íŒ… ê¸°ë¡ ì´ˆê¸°í™”
    if st.button("ì±„íŒ… ê¸°ë¡ ì´ˆê¸°í™”", type='primary'):
        st.session_state.messages = [
            {"role": "assistant", "content": say_hi_to_user}
        ]
        st.rerun()

with chat_col1:
    if 'messages' not in st.session_state:
        st.session_state.messages = [
            {"role": "assistant", "content": say_hi_to_user}
        ]
    # í•„ìˆ˜ ì •ë³´ê°€ ì…ë ¥ë˜ì§€ ì•Šì•˜ì„ ê²½ìš° ì˜¤ë¥˜ ë©”ì‹œì§€ ì¶œë ¥
    if not (user_age and visit_dates and visit_times and visit_region):
        st.error("ì‚¬ìš©ì ì •ë³´(ì—°ë ¹ëŒ€, ë°©ë¬¸ ë‚ ì§œ, ì‹œê°„, ì§€ì—­)ê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤. \nì™¼ìª½ ì‚¬ì´ë“œë°”ì—ì„œ ì •ë³´ë¥¼ ì…ë ¥í•´ ì£¼ì„¸ìš”.")
        st.stop()  # ì´í›„ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ì§€ ì•Šë„ë¡ ì¤‘ë‹¨


    for message in st.session_state.messages:
        avatar = "ğŸ§‘ğŸ»" if message['role'] == 'user' else botImgPath
        with st.chat_message(message['role'], avatar=avatar):
            st.markdown(message['content'])

    if user_input:
        st.session_state.messages.append({"role": "user", "content": user_input})
        with st.chat_message("user", avatar="ğŸ§‘ğŸ»"):
            st.markdown(user_input)

        # ì¶”ì²œ ìƒì„± ì¤‘ ìŠ¤í”¼ë„ˆ
        with st.spinner("ë§›ì§‘ ì°¾ëŠ” ì¤‘..."):
            query_text = (
                f"ì§ˆë¬¸: {user_input}\n\n"
                f"user_name: {user_name}\n"
                f"user_age: {user_age}\n"
                f"visit_region: {visit_region}\n"
                f"visit_month: {visit_month}\n"
                f"visit_times: {visit_times}"
            )
            # chain.invokeì—ì„œ ê°œë³„ ë³€ìˆ˜ë¡œ ì „ë‹¬
            assistant_response = rag_chain.invoke(query_text)

        # Assistant ì‘ë‹µ ê¸°ë¡ì— ì¶”ê°€ ë° ì¶œë ¥
        st.session_state.messages.append({"role": "assistant", "content": assistant_response})
        with st.chat_message("assistant", avatar=botImgPath):
            st.markdown(assistant_response)  
