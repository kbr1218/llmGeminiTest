# chat.py
import streamlit as st
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_chroma import Chroma
# from langchain_community.vectorstores import Chroma

from langchain_teddynote import logging
from langchain_google_genai import ChatGoogleGenerativeAI

from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.prompts import ChatPromptTemplate
from langchain_core.documents import Document

from dotenv import load_dotenv
import os

# í˜ì´ì§€ ì œëª© ì„¤ì •
st.set_page_config(page_title="main", page_icon="ğŸ’¬", layout="wide",
                   initial_sidebar_state='expanded')

from pages.subpages import sidebar, chat_search

# CSS íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°
with open('style/chat_page.css', encoding='utf-8') as css_file:
    st.markdown(f"<style>{css_file.read()}</style>", unsafe_allow_html=True)

# ì‚¬ì´ë“œë°”
with st.sidebar:
    sidebar.show_sidebar()


##########################
### 00. í™˜ê²½ë³€ìˆ˜ ë¡œë“œ ###
load_dotenv()
GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')
# langsmith ì¶”ì  ì„¤ì •
logging.langsmith("bigcon_langchain_test")


### 1. HuggingFace ì„ë² ë”© ìƒì„± ###
embeddings  = HuggingFaceEmbeddings(model_name="jhgan/ko-sroberta-multitask")
test_embedding = embeddings.embed_query("ì‚°ì§€ ë§›ì§‘")


## 2. Chroma ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ###
vectorstore = Chroma(persist_directory="./database_1000", embedding_function=embeddings)

## 3. ì‚¬ìš©ì ì •ë³´ ê¸°ë°˜ ì§€ì—­ í•„í„°ë§ ###
user_name = st.session_state.get('user_name', [])
user_age = st.session_state.get('age', [])
visit_dates = st.session_state.get('visit_dates', [])
visit_times = st.session_state.get('visit_times', [])
visit_region = st.session_state.get('region', [])

# í•„í„° ì¡°ê±´ êµ¬ì„±
region_filter = {"area": {"$in": visit_region}}
print(f"í•„í„°ë§ëœ ì§€ì—­: {visit_region}")
print(f"í•„í„° ì¡°ê±´: {region_filter}")

## 4. í•„í„°ë¥¼ ì ìš©í•˜ì—¬ ê²€ìƒ‰ê¸° ìƒì„± ###
retriever = vectorstore.as_retriever(
    search_type="mmr",   
    search_kwargs={"k": 8, "fetch_k": 10, "filters": region_filter}
)

## 5. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„¤ì • (ìˆ˜ì • í•„ìš”: ë‚ ì”¨ì— ê¸°ë°˜í•˜ì—¬ ëŒ€ë‹µí•˜ë„ë¡ ìˆ˜ì •) ###
template = """
[context]: {context}
---
[ì§ˆì˜]: {query}
---
[ì˜ˆì‹œ]
ì„ íƒí•˜ì‹  ì œì£¼ë„ [ì„ íƒí•œ ì§€ì—­]ì— ìœ„ì¹˜í•œ ë§›ì§‘ì„ ì¶”ì²œí•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤!
[ì„ íƒí•œ ë°©ë¬¸ ì‹œê°„]ì— ë°©ë¬¸í•  ë§Œí•œ [ì•„ì¹¨ì‹ì‚¬] ë§›ì§‘ ì°¾ìœ¼ì‹œëŠ”êµ°ìš”.  
[visit_datesì˜ month]ì˜ ì˜¤ì „ì˜ í‰ê·  ê¸°ì˜¨ì€ ì•½ 00.0ë„ì…ë‹ˆë‹¤.
[ì‹ë‹¹ì´ë¦„]ì˜ [3ì›”] ì˜¤ì „(5ì‹œ-11ì‹œ) ë°©ë¬¸ìœ¨ì€ ì•½ 00.00%ë¡œ ë†’ì€ í¸ì…ë‹ˆë‹¤.

ì¶”ì²œ ì´ìœ : {context}  # ë¬¸ì„œì—ì„œ ê°€ì ¸ì˜¨ ë°ì´í„°ë¥¼ í¬í•¨

ì¶”ê°€ ì •ë³´:
ë‹¹ì‹ ì€ ì£¼ì–´ì§„ [context]ì™€ í•„í„° ì¡°ê±´ì— ë§ê²Œ ì‘ë‹µí•´ì•¼ í•©ë‹ˆë‹¤.
í•„í„°ëœ ì§€ì—­ê³¼ ë¬¸ì„œì— ë”°ë¼ ë§ì¶¤í˜• ë§›ì§‘ì„ 3~5ê°œ ì¶”ì²œí•˜ê³ , ì´ìœ ë¥¼ ë°ì´í„° ê¸°ë°˜ìœ¼ë¡œ ì„¤ëª…í•˜ì„¸ìš”.
"""

prompt = ChatPromptTemplate.from_template(template)

### 6. Google Gemini ëª¨ë¸ ìƒì„± ###
@st.cache_resource
def load_model():
    system_instruction = (
        "ë‹¹ì‹ ì€ ì œì£¼ë„ ì—¬í–‰ê°ì—ê²Œ ì œì£¼ë„ ë§›ì§‘ì„ ì¶”ì²œí•˜ëŠ” ì¹œì ˆí•œ ì œì£¼ë„Â°C ì±—ë´‡ì…ë‹ˆë‹¤. "
        "ê±°ì§“ë§ì„ í•  ìˆ˜ ì—†ìœ¼ë©°, ì£¼ì–´ì§„ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì–˜ê¸°í•˜ì„¸ìš”."
    )
    model = ChatGoogleGenerativeAI(
        model="gemini-1.5-flash", temperature=0, max_tokens=5000, system_instruction=system_instruction
    )
    print("model loaded...")
    return model
model = load_model()

### 7. ê²€ìƒ‰ ê²°ê³¼ ë³‘í•© í•¨ìˆ˜ ###
def merge_pages(pages):
    merged = "\n\n".join([page.page_content for page in pages if page.page_content])
    for page in pages:
        print(f"ê²€ìƒ‰ëœ ë¬¸ì„œ: {page.metadata['area']}")
    return merged

## 8. LangChain ì²´ì¸ êµ¬ì„± ###
chain = (
    {"query": RunnablePassthrough(),
     "context": retriever | merge_pages,
     "user_name": RunnablePassthrough(),
     "user_age": RunnablePassthrough(),
     "visit_times": RunnablePassthrough(),
     "visit_region": RunnablePassthrough(),
     "visit_dates": RunnablePassthrough()}
    | prompt
    | load_model()
    | StrOutputParser()
)

### 9. Streamlit UI ###
st.title("chat page")
st.divider()

user_input = st.chat_input(
    placeholder="ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”. (ì˜ˆ: ì¶”ìë„ì— ìˆëŠ” ë§›ì§‘ì„ ì•Œë ¤ì¤˜)",
    max_chars=150
)

chat_col1, search_col2 = st.columns([2, 1])
with chat_col1:
    if 'messages' not in st.session_state:
        st.session_state.messages = [
            {"role": "assistant", "content": """ì•ˆë…•í•˜ì„¸ìš”!  
            ì œì£¼ë„ì˜ ì§€ì—­/ì‹œê°„ë³„ ê¸°ì˜¨ ë°ì´í„°ì— ê¸°ë°˜í•˜ì—¬ ì¸ê¸°ìˆëŠ” ë§›ì§‘ì„ ì°¾ì•„ë“œë¦´ **ì¹œì ˆí•œ ì œì£¼ë„Â°C**ì…ë‹ˆë‹¤.  
            ê¶ê¸ˆí•œ ê²Œ ìˆë‹¤ë©´ ì–¸ì œë“  ì§ˆë¬¸í•´ì£¼ì„¸ìš”."""}
        ]

    for message in st.session_state.messages:
        avatar = "ğŸ˜Š" if message['role'] == 'user' else "ğŸŠ"
        with st.chat_message(message['role'], avatar=avatar):
            st.markdown(message['content'])

    if user_input:
        st.session_state.messages.append({"role": "user", "content": user_input})
        with st.chat_message("user", avatar="ğŸ˜Š"):
            st.markdown(user_input)

        with st.spinner("ë§›ì§‘ ì°¾ëŠ” ì¤‘..."):
            assistant_response = chain.invoke(user_input)

        st.session_state.messages.append({"role": "assistant", "content": assistant_response})
        with st.chat_message("assistant", avatar="ğŸŠ"):
            st.markdown(assistant_response)

with search_col2:
    chat_search.show_search_restaurant()

### 01. FAISS ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ###
# embedding_model = HuggingFaceEmbeddings(model_name="jhgan/ko-sroberta-multitask")
# vector_db = FAISS.load_local("sample_1000_from_gpt",
#                              embedding_model,
#                              allow_dangerous_deserialization=True)


# ### 02. ì‚¬ìš©ì ì •ë³´ ê¸°ë°˜ ì§€ì—­ í•„í„°ë§ ###
# visit_region = st.session_state.get('region', [])

# # ë©”íƒ€ ë°ì´í„°ì—ì„œ ì§€ì—­ì— ë§ëŠ” ë°ì´í„° í•„í„°ë§
# def filter_by_region(region_list, vector_db):
#     if not region_list:
#         return list(vector_db.docstore._dict.values())  # ì§€ì—­ ì„ íƒì´ ì—†ì„ ë•Œ ëª¨ë“  ë°ì´í„° ë°˜í™˜
    
#     filtered_results = []
#     for doc in vector_db.docstore._dict.values():
#         if 'ì§€ì—­' in doc.metadata and doc.metadata['ì§€ì—­'] in region_list:
#             filtered_results.append(doc)
#     return filtered_results

# # # ì§€ì—­ì— ë§ëŠ” ë°ì´í„° í•„í„°ë§ ìˆ˜í–‰
# # filtered_docs = filter_by_region(visit_region, vector_db)

# # # í•„í„°ë§ëœ ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìƒˆ ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
# # if filtered_docs:
# #     # filtered_docsê°€ ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸ì¼ ê²½ìš° ì²˜ë¦¬
# #     documents = [Document(page_content=doc, metadata={"ì§€ì—­": visit_region}) if isinstance(doc, str) else doc for doc in filtered_docs]
    
# #     filtered_vector_db = FAISS.from_documents(
# #         [doc.page_content for doc in documents],  # page_contentëŠ” ë¬¸ìì—´ì´ ë˜ì–´ì•¼ í•¨
# #         embedding_model,
# #         metadatas=[doc.metadata for doc in documents]  # metadataë„ ê° ë¬¸ì„œì˜ ë©”íƒ€ë°ì´í„°
# #     )
# # else:
# #     st.write("ì„ íƒí•œ ì§€ì—­ì— í•´ë‹¹í•˜ëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
# #     filtered_vector_db = None



# ### 03. ê²€ìƒ‰ê¸° ìƒì„± ###
# def search_vector_db(query, vector_db):
#     if not query:
#         return "ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”."
#     retriever = vector_db.as_retriever()
#     result = retriever.get_relevant_documents(query)
#     return result


# ### 04. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„¤ì • ###
# template = """
# [context]: {context}
# ---
# [ì§ˆì˜]: {query}
# ---
# [ì˜ˆì‹œ]
# ì„ íƒí•˜ì‹  ì œì£¼ë„ [ì„ íƒí•œ ì§€ì—­]ì— ìœ„ì¹˜í•œ ë§›ì§‘ì„ ì¶”ì²œí•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤!
# [ì„ íƒí•œ ë°©ë¬¸ ì‹œê°„]ì— ë°©ë¬¸í•  ë§Œí•œ [ì•„ì¹¨ì‹ì‚¬] ë§›ì§‘ ì°¾ìœ¼ì‹œëŠ”êµ°ìš”.  
# [visit_datesì˜ month]ì˜ ì˜¤ì „ì˜ í‰ê·  ê¸°ì˜¨ì€ ì•½ 00.0ë„ì…ë‹ˆë‹¤.
# [ì‹ë‹¹ì´ë¦„]ì˜ [3ì›”] ì˜¤ì „(5ì‹œ-11ì‹œ) ë°©ë¬¸ìœ¨ì€ ì•½ 00.00%ë¡œ ë†’ì€ í¸ì…ë‹ˆë‹¤.
# ì¶”ì²œ ì´ìœ :
# -------
# ì¶”ê°€ ì •ë³´:
# ë‹¹ì‹ ì€ ì£¼ì–´ì§„ [context]ì™€ í•„í„° ì¡°ê±´ì— ë§ê²Œ ì‘ë‹µí•´ì•¼ í•©ë‹ˆë‹¤.

# ë°ì´í„° ì„¤ëª…:
# ê¸°ì¤€ë…„ì›”-2023ë…„ 1ì›”~12ì›”
# ì—…ì¢…-ìš”ì‹ê´€ë ¨ 30ê°œ ì—…ì¢…ìœ¼ë¡œ êµ¬ë¶„
# ì§€ì—­-ì œì£¼ë„ë¥¼ 10ê°œì˜ ì§€ì—­ìœ¼ë¡œ êµ¬ë¶„(ë™ë¶€/ì„œë¶€/ë‚¨ë¶€/ë¶ë¶€/ì‚°ì§€/ê°€íŒŒë„/ë§ˆë¼ë„/ë¹„ì–‘ë„/ìš°ë„/ì¶”ìë„)
# ì£¼ì†Œ-ê°€ë§¹ì  ì£¼ì†Œ
# ì›”ë³„_ì—…ì¢…ë³„_ì´ìš©ê±´ìˆ˜_ìˆœìœ„-ì›”ë³„ ì—…ì¢…ë³„ ì´ìš©ê±´ìˆ˜ ë¶„ìœ„ìˆ˜ êµ¬ê°„ì„ 6ê°œ êµ¬ê°„ìœ¼ë¡œ ì§‘ê³„ ì‹œ í•´ë‹¹ ê°€ë§¹ì ì˜ ì´ìš©ê±´ìˆ˜ê°€ í¬í•¨ë˜ëŠ” ë¶„ìœ„ìˆ˜ êµ¬ê°„ * 1:ìƒìœ„10%ì´í•˜ 2:ìƒìœ„10~25% 3:ìƒìœ„25~50% 4:ìƒìœ„50~75% 5:ìƒìœ„75~90% 6:ìƒìœ„90% ì´ˆê³¼(í•˜ìœ„10%ì´í•˜) * ìƒìœ„ 30% ë§¤ì¶œ ê°€ë§¹ì  ë‚´ ë¶„ìœ„ìˆ˜ êµ¬ê°„ì„
# ì›”ë³„_ì—…ì¢…ë³„_ì´ìš©ê¸ˆì•¡_ìˆœìœ„-ì›”ë³„ ì—…ì¢…ë³„ ì´ìš©ê¸ˆì•¡ ë¶„ìœ„ìˆ˜ êµ¬ê°„ì„ 6ê°œ êµ¬ê°„ìœ¼ë¡œ ì§‘ê³„ ì‹œ í•´ë‹¹ ê°€ë§¹ì ì˜ ì´ìš©ê¸ˆì•¡ì´ í¬í•¨ë˜ëŠ” ë¶„ìœ„ìˆ˜ êµ¬ê°„ * 1:ìƒìœ„10%ì´í•˜ 2:ìƒìœ„10~25% 3:ìƒìœ„25~50% 4:ìƒìœ„50~75% 5:ìƒìœ„75~90% 6:ìƒìœ„90% ì´ˆê³¼(í•˜ìœ„10%ì´í•˜) * ìƒìœ„ 30% ë§¤ì¶œ ê°€ë§¹ì  ë‚´ ë¶„ìœ„ìˆ˜ êµ¬ê°„ì„
# ê±´ë‹¹_í‰ê· _ì´ìš©ê¸ˆì•¡_ìˆœìœ„-ì›”ë³„ ì—…ì¢…ë³„ ê±´ë‹¹í‰ê· ì´ìš©ê¸ˆì•¡ ë¶„ìœ„ìˆ˜ êµ¬ê°„ì„ 6ê°œ êµ¬ê°„ìœ¼ë¡œ ì§‘ê³„ ì‹œ í•´ë‹¹ ê°€ë§¹ì ì˜ ê±´ë‹¹í‰ê· ì´ìš©ê¸ˆì•¡ì´ í¬í•¨ë˜ëŠ” ë¶„ìœ„ìˆ˜ êµ¬ê°„ * 1:ìƒìœ„10%ì´í•˜ 2:ìƒìœ„10~25% 3:ìƒìœ„25~50% 4:ìƒìœ„50~75% 5:ìƒìœ„75~90% 6:ìƒìœ„90% ì´ˆê³¼(í•˜ìœ„10%ì´í•˜) * ìƒìœ„ 30% ë§¤ì¶œ ê°€ë§¹ì  ë‚´ ë¶„ìœ„ìˆ˜ êµ¬ê°„ì„
# í˜„ì§€ì¸_ì´ìš©_ê±´ìˆ˜_ë¹„ì¤‘-ê³ ê° ìíƒ ì£¼ì†Œê°€ ì œì£¼ë„ì¸ ê²½ìš°ë¥¼ í˜„ì§€ì¸ìœ¼ë¡œ ì •ì˜
# """
# prompt = ChatPromptTemplate.from_template(template)


# ### 05. Google Gemini ëª¨ë¸ ìƒì„± ###
# @st.cache_resource
# def load_model():
#     system_instruction = "ë‹¹ì‹ ì€ ì œì£¼ë„ ì—¬í–‰ê°ì—ê²Œ ì œì£¼ë„ ë§›ì§‘ì„ ì¶”ì²œí•˜ëŠ” ì¹œì ˆí•œ ì œì£¼ë„Â°C ì±—ë´‡ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”."
#     model = ChatGoogleGenerativeAI(model="gemini-1.5-flash", temperature=0.2, max_tokens=5000, system_instruction=system_instruction)
#     print("model loaded...")
#     return model

# model = load_model()


# ### 06. ê²€ìƒ‰ ê²°ê³¼ ë³‘í•© í•¨ìˆ˜ ###
# def merge_pages(pages):
#     merged = "\n\n".join([page.page_content for page in pages if page.page_content])
#     return merged


# ### 07. LangChain ì²´ì¸ êµ¬ì„± ###
# chain = (
#     {"query": RunnablePassthrough(),
#      "context": search_vector_db | merge_pages,
#      "visit_region": RunnablePassthrough(),
#      "visit_dates": RunnablePassthrough(),
#     }
#     | prompt
#     | model
#     | StrOutputParser()
# )


# ### 8. streamlit UI ###
# st.title("chat page")
# st.divider()

# user_input = st.chat_input(
#     placeholder="ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”. (ì˜ˆ: ì¶”ìë„ì— ìˆëŠ” ë§›ì§‘ì„ ì•Œë ¤ì¤˜)",
#     max_chars=150
# )

# chat_col1, search_col2 = st.columns([2, 1])

# with chat_col1:
#     # ëŒ€í™” ì´ë ¥ ì´ˆê¸°í™” ë° ì²« ë²ˆì§¸ ë©”ì‹œì§€
#     if 'messages' not in st.session_state:
#         st.session_state.messages = [
#             {"role": "assistant", "content": """ì•ˆë…•í•˜ì„¸ìš”!  
#             ì œì£¼ë„ì˜ ì§€ì—­/ì‹œê°„ë³„ ê¸°ì˜¨ ë°ì´í„°ì— ê¸°ë°˜í•˜ì—¬ ì¸ê¸°ìˆëŠ” ë§›ì§‘ì„ ì°¾ì•„ë“œë¦´ **ì¹œì ˆí•œ ì œì£¼ë„â„ƒ**ì…ë‹ˆë‹¤.  
#             ê¶ê¸ˆí•œ ê²Œ ìˆë‹¤ë©´ ì–¸ì œë“  ì§ˆë¬¸í•´ì£¼ì„¸ìš”."""}
#         ]

#     # ì´ì „ ì±„íŒ… ê¸°ë¡ ì¶œë ¥
#     for message in st.session_state.messages:
#         avatar = "ğŸ˜Š" if message['role'] == 'user' else "ğŸŠ"
#         with st.chat_message(message['role'], avatar=avatar):
#             st.markdown(message['content'])

#     # if filtered_vector_db:
#         # ì‚¬ìš©ì ì…ë ¥
#         if user_input :
#             search_results = search_vector_db(user_input, filtered_vector_db)

#             st.session_state.messages.append({"role":"user", "content":user_input})
#             with st.chat_message("user", avatar="ğŸ˜Š"):
#                 st.markdown(user_input)

#             # ì¶”ì²œ ìƒì„± ì¤‘ ìŠ¤í”¼ë„ˆ
#             with st.spinner("ë§›ì§‘ ì°¾ëŠ” ì¤‘..."):
#                 assistant_response = chain.invoke(user_input)

#             # Assistant ì‘ë‹µ ê¸°ë¡ì— ì¶”ê°€ ë° ì¶œë ¥
#             st.session_state.messages.append({"role": "assistant", "content": assistant_response})
#             with st.chat_message("assistant", avatar="ğŸŠ"):
#                 st.markdown(assistant_response)

# with search_col2:
#     chat_search.show_search_restaurant()


# HuggingFace ì„ë² ë”© ìƒì„±
# embeddings = HuggingFaceEmbeddings(model_name="jhgan/ko-sroberta-multitask")
# test_embedding = embeddings.embed_query("ì‚°ì§€ ë§›ì§‘")

# ## 1. Chroma ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ (í…ŒìŠ¤íŠ¸ìš© database_1000ì—ì„œ ë¶ˆëŸ¬ì˜´ ë‚˜ì¤‘ì— ìˆ˜ì • í•„ìš”) ###
# vectorstore = Chroma(persist_directory="./database_1000", embedding_function=embeddings)

# ## 2. ì‚¬ìš©ì ì •ë³´ ê¸°ë°˜ ì§€ì—­ í•„í„°ë§ ###
# user_name = st.session_state.get('user_name', [])
# user_age = st.session_state.get('age', [])
# visit_dates = st.session_state.get('visit_dates', [])
# visit_times = st.session_state.get('visit_times', [])
# visit_region = st.session_state.get('region', [])

# í•„í„° ì¡°ê±´ êµ¬ì„±
# region_filter = {
#     "area": {"$in": visit_region}
# }

# print(f"í•„í„°ë§ëœ ì§€ì—­: {visit_region}")
# print(f"í•„í„° ì¡°ê±´: {region_filter}")

# ## 3. í•„í„°ë¥¼ ì ìš©í•˜ì—¬ ê²€ìƒ‰ê¸° ìƒì„± ###
# retriever = vectorstore.as_retriever(search_type="mmr",   #"mmr"
#                                      search_kwargs={"k": 8,            # K: kê°œì˜ ë¬¸ì„œ ê²€ìƒ‰
#                                                     "fetch_k": 10,
#                                                     "filters":region_filter}) 


# ## 4. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„¤ì • (ìˆ˜ì • í•„ìš”: ë‚ ì”¨ì— ê¸°ë°˜í•˜ì—¬ ëŒ€ë‹µí•˜ë„ë¡ ìˆ˜ì •) ###
# template = """
# [context]: {context}
# ---
# [ì§ˆì˜]: {query}
# ---
# [ì˜ˆì‹œ]
# ì„ íƒí•˜ì‹  ì œì£¼ë„ [ì„ íƒí•œ ì§€ì—­]ì— ìœ„ì¹˜í•œ ë§›ì§‘ì„ ì¶”ì²œí•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤!
# [ì„ íƒí•œ ë°©ë¬¸ ì‹œê°„]ì— ë°©ë¬¸í•  ë§Œí•œ [ì•„ì¹¨ì‹ì‚¬] ë§›ì§‘ ì°¾ìœ¼ì‹œëŠ”êµ°ìš”.  
# [visit_datesì˜ month]ì˜ ì˜¤ì „ì˜ í‰ê·  ê¸°ì˜¨ì€ ì•½ 00.0ë„ì…ë‹ˆë‹¤.
# [ì‹ë‹¹ì´ë¦„]ì˜ [3ì›”] ì˜¤ì „(5ì‹œ-11ì‹œ) ë°©ë¬¸ìœ¨ì€ ì•½ 00.00%ë¡œ ë†’ì€ í¸ì…ë‹ˆë‹¤.

# ì¶”ì²œ ì´ìœ : {context}  # ë¬¸ì„œì—ì„œ ê°€ì ¸ì˜¨ ë°ì´í„°ë¥¼ í¬í•¨

# ì¶”ê°€ ì •ë³´:
# ---
# ë‹¹ì‹ ì€ ì£¼ì–´ì§„ [context]ì™€ í•„í„° ì¡°ê±´ì— ë§ê²Œ ì‘ë‹µí•´ì•¼ í•©ë‹ˆë‹¤.
# í•„í„°ëœ ì§€ì—­ê³¼ ë¬¸ì„œì— ë”°ë¼ ë§ì¶¤í˜• ë§›ì§‘ì„ 3~5ê°œ ì¶”ì²œí•˜ê³ , ì´ìœ ë¥¼ ë°ì´í„° ê¸°ë°˜ìœ¼ë¡œ ì„¤ëª…í•˜ì„¸ìš”.

# ë°ì´í„°ì— ëŒ€í•œ ì„¤ëª…ì…ë‹ˆë‹¤. ì‚¬ìš©ìê°€ ìš”ì²­í•˜ëŠ” ì§ˆë¬¸ì—ì„œ ì§€ì—­ ì •ë³´ë¥¼ ì°¾ì•„ area ë³€ìˆ˜ì—ì„œ í•„í„°ë§í•œ í›„ ë‹µë³€í•˜ì„¸ìš”.
# YM: ê¸°ì¤€ì—°ì›”(1ì›”~12ì›”), MCT_NM: ê°€ë§¹ì ëª…, MCT_TYPE: ìš”ì‹ê´€ë ¨ 30ê°œ ì—…ì¢…, temp_05_11: 5ì‹œ 11ì‹œ í‰ê·  ê¸°ì˜¨, temp_12_13: 12ì‹œ 13ì‹œ í‰ê·  ê¸°ì˜¨, temp_14_17: 14ì‹œ 17ì‹œ í‰ê·  ê¸°ì˜¨, temp_18_22: 18ì‹œ 22ì‹œ í‰ê·  ê¸°ì˜¨, temp_23_04: 23ì‹œ 4ì‹œ í‰ê·  ê¸°ì˜¨, TEMP_AVG: ì›”(YM) í‰ê·  ê¸°ì˜¨, area: ì œì£¼ë„ë¥¼ 10ê°œì˜ ì§€ì—­ìœ¼ë¡œ êµ¬ë¶„: ë™ë¶€/ì„œë¶€/ë‚¨ë¶€/ë¶ë¶€/ì‚°ì§€/ê°€íŒŒë„/ë§ˆë¼ë„/ë¹„ì–‘ë„/ìš°ë„/ì¶”ìë„, ADDR: ê°€ë§¹ì  ì£¼ì†Œ, RANK_CNT: ì›”ë³„ ì—…ì¢…ë³„ ì´ìš©ê±´ìˆ˜ ë¶„ìœ„ìˆ˜ êµ¬ê°„ì„ 6ê°œ êµ¬ê°„ìœ¼ë¡œ ì§‘ê³„ ì‹œ í•´ë‹¹ ê°€ë§¹ì ì˜ ì´ìš©ê±´ìˆ˜ê°€ í¬í•¨ë˜ëŠ” ë¶„ìœ„ìˆ˜ êµ¬ê°„ * 1:ìƒìœ„10%ì´í•˜ 2:ìƒìœ„10~25% 3:ìƒìœ„25~50% 4:ìƒìœ„50~75% 5:ìƒìœ„75~90% 6:ìƒìœ„90% ì´ˆê³¼(í•˜ìœ„10%ì´í•˜) * ìƒìœ„ 30% ë§¤ì¶œ ê°€ë§¹ì  ë‚´ ë¶„ìœ„ìˆ˜ êµ¬ê°„ì„, RANK_AMT: ì›”ë³„ ì—…ì¢…ë³„ ì´ìš©ê¸ˆì•¡ ë¶„ìœ„ìˆ˜ êµ¬ê°„ì„ 6ê°œ êµ¬ê°„ìœ¼ë¡œ ì§‘ê³„ ì‹œ í•´ë‹¹ ê°€ë§¹ì ì˜ ì´ìš©ê¸ˆì•¡ì´ í¬í•¨ë˜ëŠ” ë¶„ìœ„ìˆ˜ êµ¬ê°„ * 1:ìƒìœ„10%ì´í•˜ 2:ìƒìœ„10~25% 3:ìƒìœ„25~50% 4:ìƒìœ„50~75% 5:ìƒìœ„75~90% 6:ìƒìœ„90% ì´ˆê³¼(í•˜ìœ„10%ì´í•˜) * ìƒìœ„ 30% ë§¤ì¶œ ê°€ë§¹ì  ë‚´ ë¶„ìœ„ìˆ˜ êµ¬ê°„ì„, RANK_MEAN: ì›”ë³„ ì—…ì¢…ë³„ ê±´ë‹¹í‰ê· ì´ìš©ê¸ˆì•¡ ë¶„ìœ„ìˆ˜ êµ¬ê°„ì„ 6ê°œ êµ¬ê°„ìœ¼ë¡œ ì§‘ê³„ ì‹œ í•´ë‹¹ ê°€ë§¹ì ì˜ ê±´ë‹¹í‰ê· ì´ìš©ê¸ˆì•¡ì´ í¬í•¨ë˜ëŠ” ë¶„ìœ„ìˆ˜ êµ¬ê°„ * 1:ìƒìœ„10%ì´í•˜ 2:ìƒìœ„10~25% 3:ìƒìœ„25~50% 4:ìƒìœ„50~75% 5:ìƒìœ„75~90% 6:ìƒìœ„90% ì´ˆê³¼(í•˜ìœ„10%ì´í•˜) * ìƒìœ„ 30% ë§¤ì¶œ ê°€ë§¹ì  ë‚´ ë¶„ìœ„ìˆ˜ êµ¬ê°„ì„, MON_UE_CNT_RAT: ì›”ìš”ì¼ ì´ìš© ê±´ìˆ˜ ë¹„ì¤‘, TUE_UE_CNT_RAT: í™”ìš”ì¼ ì´ìš© ê±´ìˆ˜ ë¹„ì¤‘, WED_UE_CNT_RAT: ìˆ˜ìš”ì¼ ì´ìš© ê±´ìˆ˜ ë¹„ì¤‘, THU_UE_CNT_RAT: ëª©ìš”ì¼ ì´ìš© ê±´ìˆ˜ ë¹„ì¤‘, FRI_UE_CNT_RAT: ê¸ˆìš”ì¼ ì´ìš© ê±´ìˆ˜ ë¹„ì¤‘, SAT_UE_CNT_RAT: í† ìš”ì¼ ì´ìš© ê±´ìˆ˜ ë¹„ì¤‘, SUN_UE_CNT_RAT: ì¼ìš”ì¼ ì´ìš© ê±´ìˆ˜ ë¹„ì¤‘, HR_5_11_UE_CNT_RAT: 5ì‹œ-11ì‹œ ì´ìš© ê±´ìˆ˜ ë¹„ì¤‘, HR_12_13_UE_CNT_RAT: 12ì‹œ-13ì‹œ ì´ìš© ê±´ìˆ˜ ë¹„ì¤‘, HR_14_17_UE_CNT_RAT: 14ì‹œ-17ì‹œ ì´ìš© ê±´ìˆ˜ ë¹„ì¤‘, HR_18_22_UE_CNT_RAT: 18ì‹œ-22ì‹œ ì´ìš© ê±´ìˆ˜ ë¹„ì¤‘, HR_23_4_UE_CNT_RAT: 23ì‹œ-4ì‹œ ì´ìš© ê±´ìˆ˜ ë¹„ì¤‘, LOCAL_UE_CNT_RAT: í˜„ì§€ì¸ ì´ìš© ê±´ìˆ˜ ë¹„ì¤‘ (ê³ ê° ìíƒ ì£¼ì†Œê°€ ì œì£¼ë„ì¸ ê²½ìš° í˜„ì§€ì¸ìœ¼ë¡œ ì •ì˜), RC_M12_MAL_CUS_CNT_RAT: ìµœê·¼ 12ê°œì›” ë‚¨ì„± íšŒì›ìˆ˜ ë¹„ì¤‘ (ê¸°ì¤€ì—°ì›” í¬í•¨ ìµœê·¼ 12ê°œì›” ì§‘ê³„í•œ ê°’), RC_M12_FME_CUS_CNT_RAT: ìµœê·¼ 12ê°œì›” ì—¬ì„± íšŒì›ìˆ˜ ë¹„ì¤‘ (ê¸°ì¤€ì—°ì›” í¬í•¨ ìµœê·¼ 12ê°œì›” ì§‘ê³„í•œ ê°’), RC_M12_AGE_UND_20_CUS_CNT_RAT: ìµœê·¼ 12ê°œì›” 20ëŒ€ ì´í•˜ íšŒì›ìˆ˜ ë¹„ì¤‘ (ê¸°ì¤€ì—°ì›” í¬í•¨ ìµœê·¼ 12ê°œì›” ì§‘ê³„í•œ ê°’), RC_M12_AGE_30_CUS_CNT_RAT: ìµœê·¼ 12ê°œì›” 30ëŒ€ íšŒì›ìˆ˜ ë¹„ì¤‘ (ê¸°ì¤€ì—°ì›” í¬í•¨ ìµœê·¼ 12ê°œì›” ì§‘ê³„í•œ ê°’), RC_M12_AGE_40_CUS_CNT_RAT: ìµœê·¼ 12ê°œì›” 40ëŒ€ íšŒì›ìˆ˜ ë¹„ì¤‘ (ê¸°ì¤€ì—°ì›” í¬í•¨ ìµœê·¼ 12ê°œì›” ì§‘ê³„í•œ ê°’), RC_M12_AGE_50_CUS_CNT_RAT: ìµœê·¼ 12ê°œì›” 40ëŒ€ íšŒì›ìˆ˜ ë¹„ì¤‘ (ê¸°ì¤€ì—°ì›” í¬í•¨ ìµœê·¼ 12ê°œì›” ì§‘ê³„í•œ ê°’), RC_M12_AGE_OVR_60_CUS_CNT_RAT: ìµœê·¼ 12ê°œì›” 60ëŒ€ ì´ìƒ íšŒì›ìˆ˜ ë¹„ì¤‘ (ê¸°ì¤€ì—°ì›” í¬í•¨ ìµœê·¼ 12ê°œì›” ì§‘ê³„í•œ ê°’)
# """

# # ìœ„ì˜ [context] ì •ë³´ ë‚´ì—ì„œ [ì§ˆì˜]ì— ëŒ€í•´ ë‹µë³€ [ì˜ˆì‹œ]ì™€ ê°™ì´ ìˆ ì–´ë¥¼ ë¶™ì—¬ì„œ ë‹µí•˜ì„¸ìš”.
# # ì‚¬ìš©ìê°€ êµ¬ì²´ì ì¸ ìˆ«ìë¥¼ ì œì‹œí•˜ì§€ ì•Šì•˜ë‹¤ë©´, 3-5ê°œì˜ ë§›ì§‘ì„ ì¶”ì²œí•´ì£¼ì„¸ìš”.
# # 'visit_region'ì€ area ë³€ìˆ˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì„ íƒë˜ì—ˆìŠµë‹ˆë‹¤. 
# # ì¶”ì²œ ì´ìœ ëŠ” êµ¬ì²´ì ì¼ ìˆ˜ë¡ ì¢‹ìŠµë‹ˆë‹¤. ì™œ ì‚¬ìš©ìì—ê²Œ ì´ëŸ° ë§›ì§‘ì„ ì¶”ì²œí–ˆëŠ”ì§€ ë¹„ì¤‘ ë°ì´í„°ë¥¼ ê·¼ê±°ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”.

# prompt = ChatPromptTemplate.from_template(template)


# ### 5. Google Gemini ëª¨ë¸ ìƒì„± ###
# @st.cache_resource
# def load_model():
#     system_instruction = "ë‹¹ì‹ ì€ ì œì£¼ë„ ì—¬í–‰ê°ì—ê²Œ ì œì£¼ë„ ë§›ì§‘ì„ ì¶”ì²œí•˜ëŠ” ì¹œì ˆí•œ ì œì£¼ë„Â°C ì±—ë´‡ì…ë‹ˆë‹¤. ê±°ì§“ë§ì„ í•  ìˆ˜ ì—†ìœ¼ë©°, ì£¼ì–´ì§„ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì–˜ê¸°í•˜ì„¸ìš”."
#     model = ChatGoogleGenerativeAI(model="gemini-1.5-flash",
#                                    temperature=0.2,
#                                    max_tokens=5000,
#                                    system_instruction=system_instruction)
#     print("model loaded...")
#     return model
# model = load_model()


# ### 6. ê²€ìƒ‰ ê²°ê³¼ ë³‘í•© í•¨ìˆ˜ ###
# def merge_pages(pages):
#     merged = "\n\n".join([page.page_content for page in pages if page.page_content])

#     # ê²€ìƒ‰ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ ì¶œë ¥ (í•„í„° í™•ì¸ìš©)
#     for page in pages:
#         print(f"ê²€ìƒ‰ëœ ë¬¸ì„œ: {page.metadata['area']}")  # ì§€ì—­ í•„í„° í™•ì¸ì„ ìœ„í•´ 'area' í•„ë“œ ì¶œë ¥
    
#     return merged
    


# ## 7. LangChain ì²´ì¸ êµ¬ì„± ###
# chain = (
#     {"query": RunnablePassthrough(),
#      "context": retriever | merge_pages,    # retrieverë¡œ ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ merge_pages í•¨ìˆ˜ì— ì „ë‹¬
#      "user_name":RunnablePassthrough(),     # RunnablePassThrough: ê°’ì„ ë³€ê²½í•˜ì§€ ì•Šê³  ê·¸ëŒ€ë¡œ í†µê³¼ì‹œí‚´
#      "user_age":RunnablePassthrough(),
#      "visit_times":RunnablePassthrough(),
#      "visit_region": RunnablePassthrough(),
#      "visit_dates": RunnablePassthrough(),
#     }
#     | prompt
#     | load_model()
#     | StrOutputParser()
# )


    #-----------------------------------------------------------

    # if user_input:
    #     # ì‚¬ìš©ì ì…ë ¥ì„ historyì— ì €ì¥
    #     st.session_state.chat_history.append({"role":"user", "content":user_input})
    #     with st.spinner("ì¶”ì²œì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤..."):
    #         # ì‹¤ì‹œê°„ spinnerë¡œ ì¤‘ê°„ ê²°ê³¼ ì¶œë ¥
    #         message_placeholder = st.empty()
    #         full_response = ""
    #         pages = retriever.get_relevant_documents(user_input)   # ê²€ìƒ‰ ê²°ê³¼
    #         context = merge_pages(pages)
    #         query = {"query":user_input, "context":context}

    #         # response_stream = chain.invoke_stream(query)    # ì‹¤ì‹œê°„ ì‘ë‹µ ë°›ê¸°

    #         # for chunk in response_stream:
    #         #     full_response += chunk.text
    #         #     message_placeholder.markdown(full_response)

    #         full_response = chain.invoke(query)
            
    #         # ìµœì¢… ì‘ë‹µ ì €ì¥
    #         st.session_state.chat_history.append({"role": "ai", "content": full_response})
    #         st.success("ì¶”ì²œ ê²°ê³¼:")
    #         st.write(full_response)
