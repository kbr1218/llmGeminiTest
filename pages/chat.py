# chat.py
import streamlit as st

from langchain_community.vectorstores import Chroma
from langchain.embeddings import HuggingFaceEmbeddings

from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.prompts import ChatPromptTemplate

from langchain_teddynote import logging
from dotenv import load_dotenv
import os

# ì±—ë´‡ ì´ë¯¸ì§€ ë§í¬ ì„ ì–¸
botImgPath = 'https://raw.githubusercontent.com/kbr1218/streamlitTest/main/imgs/dolhareubang3.png'

# í˜ì´ì§€ ì œëª© ì„¤ì •
st.set_page_config(page_title="chat", page_icon="ğŸ’¬", layout="wide",
                   initial_sidebar_state='expanded')

from pages.subpages import sidebar, chat_search

# ì‚¬ì´ë“œë°”
with st.sidebar:
    sidebar.show_sidebar()


##########################
### 00. í™˜ê²½ë³€ìˆ˜ ë¡œë“œ ###
load_dotenv()
GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')
# langsmith ì¶”ì  ì„¤ì •
logging.langsmith("bigcon_langchain_test")


### 1. HuggingFace ì„ë² ë”© ìƒì„± ###
embeddings  = HuggingFaceEmbeddings(model_name="jhgan/ko-sroberta-multitask")


## 2. Chroma ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ###
vectorstore = Chroma(persist_directory="./sample_1500_vectorstore", embedding_function=embeddings)


## 3. ì‚¬ìš©ì ì •ë³´ ê¸°ë°˜ ì§€ì—­ í•„í„°ë§ ###
user_name = st.session_state.get('user_name', [])
user_age = st.session_state.get('age', [])
visit_dates = st.session_state.get('visit_dates', [])
visit_times = st.session_state.get('visit_times', [])
visit_region = st.session_state.get('region', [])


## 4. ê²€ìƒ‰ê¸° ìƒì„± ###
retriever = vectorstore.as_retriever(
    search_type="mmr",   
    search_kwargs={"k": 10,              # ë°˜í™˜í•  ë¬¸ì„œ ìˆ˜ (default: 4)
                   "fetch_k": 50,        # MMR ì•Œê³ ë¦¬ì¦˜ì— ì „ë‹¬í•  ë¬¸ì„œ ìˆ˜
                   "lambda_mult": 0.5}   # ê²°ê³¼ ë‹¤ì–‘ì„± ì¡°ì ˆ (default: 0.5)
)


## 5. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„¤ì • (ìˆ˜ì • í•„ìš”: ë‚ ì”¨ì— ê¸°ë°˜í•˜ì—¬ ëŒ€ë‹µí•˜ë„ë¡ ìˆ˜ì •) ###
template = """
You are an assistant for question-answering tasks named 'ì¹œì ˆí•œ ì œì£¼ë„Â°C', which recommends good restaurants in Jeju Island based on the given temperature and restaurant infomation data.

Use the following pieces of retrieved context to answer the question.
If you don't know the answer, just say that you don't know. Recommend three palces maximum and keep the answer concise.

Restaurant Recommendations Criteria: restaurants with a similar temperature in the month of {visit_dates} and the time zone {visit_times} where the user wants to visit has a high percentage/priority of use.

[context]: {context}
---
[ì§ˆì˜]: {query}
---
[ì˜ˆì‹œ ì‘ë‹µ]
{user_name}ë‹˜, {visit_dates}ì›” {visit_region} {visit_times} ë§›ì§‘ ì¶”ì²œë“œë¦½ë‹ˆë‹¤!
{visit_dates}ì›” {visit_region}ì˜ í‰ê·  ê¸°ì˜¨ì€ [í‰ê· ê¸°ì˜¨]Â°Cì´ê³ , {visit_times} ì‹œê°„ëŒ€ì˜ í‰ê·  ê¸°ì˜¨ì€ [ì‹œê°„ëŒ€ í‰ê· ê¸°ì˜¨]Â°Cì…ë‹ˆë‹¤. ì´ ì‹œê°„ëŒ€ ì´ìš© ê±´ìˆ˜ ë¹„ì¤‘ì´ ë†’ì€ ê³³ì„ ê³ ë ¤í•˜ì—¬ ì¶”ì²œë“œë¦´ê²Œìš”.

ì¶”ì²œ ë§›ì§‘:
1. [**ê°€ë§¹ì ëª…**]:
- {visit_dates}ì›” {visit_region} ì§€ì—­ì˜ í‰ê·  ê¸°ì˜¨ê³¼ ìœ ì‚¬í•œ ì‹œê¸°ì— {visit_times} ì‹œê°„ëŒ€ ì´ìš© ê±´ìˆ˜ ë¹„ì¤‘ì´ [ì´ìš©ê±´ìˆ˜ë¹„ì¤‘]%ë¡œ ë†’ì•˜ìŠµë‹ˆë‹¤.
- [ì›”ë³„-ì—…ì¢…ë³„ ì´ìš©ê±´ìˆ˜ ìˆœìœ„]ìœ„ë¥¼ ê¸°ë¡í–ˆìœ¼ë©°, ì—°ë ¹ëŒ€ {user_age}ì˜ ë°©ë¬¸ ë¹„ìœ¨ì´ [ì—°ë ¹ë³„ íšŒì›ìˆ˜ ë¹„ì¤‘]%ë¡œ ë¹„ìŠ·í•œ ì—°ë ¹ëŒ€ ê³ ê°ì´ ë§ì´ ì°¾ìŠµë‹ˆë‹¤.

----
[ë°ì´í„° ì„¤ëª…]
{user_age}: ì‚¬ìš©ìì˜ ì—°ë ¹ëŒ€,
{visit_dates}: ì‚¬ìš©ìê°€ ì œì£¼ë„ë¥¼ ë°©ë¬¸í•˜ëŠ” ê¸°ê°„,
{visit_times}: ì‚¬ìš©ìê°€ ë§›ì§‘ì„ ë°©ë¬¸í•  ì‹œê°„,
{visit_region}: ì‚¬ìš©ìê°€ ë°©ë¬¸í•˜ëŠ” ì œì£¼ë„ ì§€ì—­,
ê¸°ì¤€ë…„ì›”-2023ë…„ 1ì›”~12ì›”,
ì—…ì¢…-ìš”ì‹ê´€ë ¨ 30ê°œ ì—…ì¢…ìœ¼ë¡œ êµ¬ë¶„ (ì—…ì¢…ì´ 'ì»¤í”¼'ì¼ ê²½ìš° 'ì¹´í˜'ë¥¼ ëœ»í•¨ ),
ì§€ì—­-ì œì£¼ë„ë¥¼ 10ê°œì˜ ì§€ì—­ìœ¼ë¡œ êµ¬ë¶„(ë™ë¶€/ì„œë¶€/ë‚¨ë¶€/ë¶ë¶€/ì‚°ì§€/ê°€íŒŒë„/ë§ˆë¼ë„/ë¹„ì–‘ë„/ìš°ë„/ì¶”ìë„),
ì£¼ì†Œ-ê°€ë§¹ì  ì£¼ì†Œ,
ì›”ë³„_ì—…ì¢…ë³„_ì´ìš©ê±´ìˆ˜_ìˆœìœ„: ì›”ë³„ ì—…ì¢…ë³„ ì´ìš©ê±´ìˆ˜ ë¶„ìœ„ìˆ˜ êµ¬ê°„ì„ 6ê°œ êµ¬ê°„ìœ¼ë¡œ ì§‘ê³„ ì‹œ í•´ë‹¹ ê°€ë§¹ì ì˜ ì´ìš©ê±´ìˆ˜ê°€ í¬í•¨ë˜ëŠ” ë¶„ìœ„ìˆ˜ êµ¬ê°„ * 1:ìƒìœ„ 10%ì´í•˜ 2:ìƒìœ„ 10~25% 3:ìƒìœ„ 25~50% 4:ìƒìœ„ 50~75% 5:ìƒìœ„ 75~90% 6:ìƒìœ„ 90% ì´ˆê³¼(í•˜ìœ„ 10%ì´í•˜),
ì›”ë³„_ì—…ì¢…ë³„_ì´ìš©ê¸ˆì•¡_ìˆœìœ„: ì›”ë³„ ì—…ì¢…ë³„ ì´ìš©ê¸ˆì•¡ ë¶„ìœ„ìˆ˜ êµ¬ê°„ì„ 6ê°œ êµ¬ê°„ìœ¼ë¡œ ì§‘ê³„ ì‹œ í•´ë‹¹ ê°€ë§¹ì ì˜ ì´ìš©ê¸ˆì•¡ì´ í¬í•¨ë˜ëŠ” ë¶„ìœ„ìˆ˜ êµ¬ê°„ * 1:ìƒìœ„ 10%ì´í•˜ 2:ìƒìœ„ 10~25% 3:ìƒìœ„ 25~50% 4:ìƒìœ„ 50~75% 5:ìƒìœ„ 75~90% 6:ìƒìœ„ 90% ì´ˆê³¼(í•˜ìœ„ 10%ì´í•˜),
ê±´ë‹¹_í‰ê· _ì´ìš©ê¸ˆì•¡_ìˆœìœ„: ì›”ë³„ ì—…ì¢…ë³„ ê±´ë‹¹í‰ê· ì´ìš©ê¸ˆì•¡ ë¶„ìœ„ìˆ˜ êµ¬ê°„ì„ 6ê°œ êµ¬ê°„ìœ¼ë¡œ ì§‘ê³„ ì‹œ í•´ë‹¹ ê°€ë§¹ì ì˜ ê±´ë‹¹ í‰ê·  ì´ìš©ê¸ˆì•¡ì´ í¬í•¨ë˜ëŠ” ë¶„ìœ„ìˆ˜ êµ¬ê°„ * 1:ìƒìœ„ 10%ì´í•˜ 2:ìƒìœ„ 10~25% 3:ìƒìœ„ 25~50% 4:ìƒìœ„ 50~75% 5:ìƒìœ„ 75~90% 6:ìƒìœ„ 90% ì´ˆê³¼(í•˜ìœ„ 10%ì´í•˜),
í˜„ì§€ì¸_ì´ìš©_ê±´ìˆ˜_ë¹„ì¤‘: ê³ ê° ìíƒ ì£¼ì†Œê°€ ì œì£¼ë„ì¸ ê²½ìš°ë¥¼ í˜„ì§€ì¸ìœ¼ë¡œ ì •ì˜
"""
prompt = ChatPromptTemplate.from_template(template)


### 6. Google Gemini ëª¨ë¸ ìƒì„± ###
# @st.cache_resource
def load_model():
    system_instruction = (
        "ë‹¹ì‹ ì€ ì œì£¼ë„ ì—¬í–‰ê°ì—ê²Œ ì œì£¼ë„ ë§›ì§‘ì„ ì¶”ì²œí•˜ëŠ” 'ì¹œì ˆí•œ ì œì£¼Â°C' ì±—ë´‡ì…ë‹ˆë‹¤. "
        "ì‚¬ìš©ìê°€ ì‚¬ì „ì— ì œê³µí•œ ë°ì´í„°(ì‚¬ìš©ì ì´ë¦„: {user_name}, ì—°ë ¹ëŒ€: {user_age}, ë°©ë¬¸ê¸°ê°„: {visit_dates}, ë°©ë¬¸ ì‹œê°„ëŒ€: {visit_times}, ë°©ë¬¸ ì§€ì—­: {visit_region})ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì–˜ê¸°í•˜ì„¸ìš”."
    )
    model = ChatGoogleGenerativeAI(
        model="gemini-1.5-flash",
        temperature=0,
        max_tokens=5000,
        system_instruction=system_instruction
    )
    print("model loaded...")
    return model


### 7. ê²€ìƒ‰ ê²°ê³¼ í•„í„°ë§ & ë³‘í•© í•¨ìˆ˜ ###
# visit_region ë°ì´í„° í•„í„°ë§
def filter_results_by_region(docs, visit_region):
    return [doc for doc in docs if doc.metadata.get('ì§€ì—­') in visit_region]

def format_docs(docs):
  return "\n\n".join(doc.page_content for doc in docs)


## 8. LangChain ì²´ì¸ êµ¬ì„± ###
rag_chain = (
  {"context": retriever
   | (lambda docs: filter_results_by_region(docs, visit_region))
   | format_docs(),
    "query":RunnablePassthrough(),
    "user_name":RunnablePassthrough(),
    "user_age":RunnablePassthrough(),
    "visit_dates":RunnablePassthrough(),
    "visit_times":RunnablePassthrough(),
    "visit_region":RunnablePassthrough()
  }
  # question(ì‚¬ìš©ìì˜ ì§ˆë¬¸) ê¸°ë°˜ìœ¼ë¡œ ì—°ê´€ì„±ì´ ë†’ì€ ë¬¸ì„œ retriever ìˆ˜í–‰ >> format_docsë¡œ ë¬¸ì„œë¥¼ í•˜ë‚˜ë¡œ ë§Œë“¦
  | prompt               # í•˜ë‚˜ë¡œ ë§Œë“  ë¬¸ì„œë¥¼ promptì— ë„˜ê²¨ì£¼ê³ 
  | load_model()         # llmì´ ì›í•˜ëŠ” ë‹µë³€ì„ ë§Œë“¦
  | StrOutputParser()
)


### 9. Streamlit UI ###
st.subheader("ğŸŠ:orange[ì œì£¼Â°C]ì—ê²Œ ì§ˆë¬¸í•˜ê¸°")
st.divider()

user_input = st.chat_input(
    placeholder="ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”. (ì˜ˆ: ì¶”ìë„ì— ìˆëŠ” ê°€ì •ì‹ ë§›ì§‘ì„ ì¶”ì²œí•´ì¤˜)",
    max_chars=150
)

chat_col1, search_col2 = st.columns([2, 1])
with search_col2:
    chat_search.show_search_restaurant()

    # ì±„íŒ… ê¸°ë¡ ì´ˆê¸°í™”
    if st.button("ì±„íŒ… ê¸°ë¡ ì´ˆê¸°í™”", type='primary'):
        st.session_state.messages = [
            {"role": "assistant", "content": """ì•ˆë…•í•˜ì„¸ìš”!  
            ì œì£¼ë„ì˜ ì§€ì—­/ì‹œê°„ë³„ ê¸°ì˜¨ ë°ì´í„°ì— ê¸°ë°˜í•˜ì—¬ ë§›ì§‘ì„ ì¶”ì²œí•˜ëŠ” :orange[**ì¹œì ˆí•œ ì œì£¼Â°C**]ì…ë‹ˆë‹¤.  
            ì–¸ì œë“ ì§€ ì§ˆë¬¸í•´ì£¼ì„¸ìš”."""}
        ]
        st.rerun()

with chat_col1:
    if 'messages' not in st.session_state:
        st.session_state.messages = [
            {"role": "assistant", "content": """ì•ˆë…•í•˜ì„¸ìš”!  
            ì œì£¼ë„ì˜ ì§€ì—­/ì‹œê°„ë³„ ê¸°ì˜¨ ë°ì´í„°ì— ê¸°ë°˜í•˜ì—¬ ë§›ì§‘ì„ ì¶”ì²œí•˜ëŠ” :orange[**ì¹œì ˆí•œ ì œì£¼Â°C**]ì…ë‹ˆë‹¤.  
            ì–¸ì œë“ ì§€ ì§ˆë¬¸í•´ì£¼ì„¸ìš”."""}
        ]
    # í•„ìˆ˜ ì •ë³´ê°€ ì…ë ¥ë˜ì§€ ì•Šì•˜ì„ ê²½ìš° ì˜¤ë¥˜ ë©”ì‹œì§€ ì¶œë ¥
    if not (user_age and visit_dates and visit_times and visit_region):
        st.error("ì‚¬ìš©ì ì •ë³´(ì—°ë ¹ëŒ€, ë°©ë¬¸ ë‚ ì§œ, ì‹œê°„, ì§€ì—­)ê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤. \nì™¼ìª½ ì‚¬ì´ë“œë°”ì—ì„œ ì •ë³´ë¥¼ ì…ë ¥í•´ ì£¼ì„¸ìš”.")
        st.stop()  # ì´í›„ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ì§€ ì•Šë„ë¡ ì¤‘ë‹¨


    for message in st.session_state.messages:
        avatar = "ğŸ§‘ğŸ»" if message['role'] == 'user' else botImgPath
        with st.chat_message(message['role'], avatar=avatar):
            st.markdown(message['content'])

    if user_input:
        st.session_state.messages.append({"role": "user", "content": user_input})
        with st.chat_message("user", avatar="ğŸ§‘ğŸ»"):
            st.markdown(user_input)

        # ì¶”ì²œ ìƒì„± ì¤‘ ìŠ¤í”¼ë„ˆ
        with st.spinner("ë§›ì§‘ ì°¾ëŠ” ì¤‘..."):
            # chain.invokeì—ì„œ ê°œë³„ ë³€ìˆ˜ë¡œ ì „ë‹¬
            assistant_response = rag_chain.invoke(user_input+f"""
                                              user_name: {user_name},
                                              user_age: {user_age},
                                              visit_region: {visit_region},
                                              visit_dates: {visit_dates},
                                              visit_times: {visit_times},
                                              chat_history: {st.session_state.messages}
                                              """)

        # Assistant ì‘ë‹µ ê¸°ë¡ì— ì¶”ê°€ ë° ì¶œë ¥
        st.session_state.messages.append({"role": "assistant", "content": assistant_response})
        with st.chat_message("assistant", avatar=botImgPath):
            st.markdown(assistant_response)  
