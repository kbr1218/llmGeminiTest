# chat.py
import streamlit as st

from langchain_community.vectorstores import Chroma
from langchain.embeddings import HuggingFaceEmbeddings

from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.prompts import ChatPromptTemplate

from langchain_teddynote import logging
from dotenv import load_dotenv
import os

# ì±—ë´‡ ì´ë¯¸ì§€ ë§í¬ ì„ ì–¸
botImgPath = 'https://raw.githubusercontent.com/kbr1218/streamlitTest/main/imgs/dolhareubang3.png'

# í˜ì´ì§€ ì œëª© ì„¤ì •
st.set_page_config(page_title="chat", page_icon="ğŸ’¬", layout="wide",
                   initial_sidebar_state='expanded')

from pages.subpages import sidebar, chat_search

# ì‚¬ì´ë“œë°”
with st.sidebar:
    sidebar.show_sidebar()


##########################
### 00. í™˜ê²½ë³€ìˆ˜ ë¡œë“œ ###
load_dotenv()
GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')
# langsmith ì¶”ì  ì„¤ì •
logging.langsmith("bigcon_langchain_test")


### 1. HuggingFace ì„ë² ë”© ìƒì„± ###
embeddings  = HuggingFaceEmbeddings(model_name="jhgan/ko-sroberta-multitask")


## 2. Chroma ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ###
vectorstore = Chroma(persist_directory="./sample_1000_vectorstore", embedding_function=embeddings)
temperature_vectorstore = Chroma(persist_directory="./temperature_vectorstore", embedding_function=embeddings)


## 3. ì‚¬ìš©ì ì •ë³´ ê¸°ë°˜ ì§€ì—­ í•„í„°ë§ ###
user_name = st.session_state.get('user_name', None)
user_age = st.session_state.get('age', None)
visit_times = st.session_state.get('visit_times', None)
visit_region = st.session_state.get('region', [])
visit_dates = st.session_state.get('visit_dates', None)
# ì›” ì •ë³´ë§Œ ì¶œë ¥
visit_month = f"{visit_dates.month}ì›”" if visit_dates else ""


### 4. ê¸°ì˜¨ ë°ì´í„° ë¡œë“œ ###
temp_retriever = temperature_vectorstore.as_retriever(
    search_type="mmr",   
    search_kwargs={"k": 5}  # ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ í•œ ê°œì˜ ë¬¸ì„œë§Œ ê°€ì ¸ì˜¤ê¸°
)

### 5. ê²€ìƒ‰ê¸° ìƒì„± ###
retriever = vectorstore.as_retriever(
    search_type="mmr",   
    search_kwargs={"k": 10,              # ë°˜í™˜í•  ë¬¸ì„œ ìˆ˜ (default: 4)
                   "fetch_k": 50,        # MMR ì•Œê³ ë¦¬ì¦˜ì— ì „ë‹¬í•  ë¬¸ì„œ ìˆ˜
                   "lambda_mult": 0.5,    # ê²°ê³¼ ë‹¤ì–‘ì„± ì¡°ì ˆ (default: 0.5),
                   'filter': {'ì§€ì—­': {'$in':visit_region}}
                   }
    # filters={"ì§€ì—­":visit_region}
)


### 6. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„¤ì • ###
template = """
You are an assistant for question-answering tasks named 'ì¹œì ˆí•œ ì œì£¼ë„Â°C' which recommends restaurants in Jeju Island based on the given data.
If you don't know the answer, just say that you don't know. Recommend three places maximum and keep the answer concise.

When starting a response, provide a summary of the relevant temperature information from the retrieved context for {visit_month} and {visit_times}, and then continue with restaurant recommendations.

Format your response in the following structure:
"{user_name}ë‹˜, {visit_month}ì›” {visit_times}ì— ë°©ë¬¸í•˜ì‹¤ {visit_region} ì§€ì—­ì˜ ë§›ì§‘ì„ ì¶”ì²œë“œë¦½ë‹ˆë‹¤."

ì¶”ì²œ ë§›ì§‘:
1. [**ê°€ë§¹ì ëª…**]:
- ì£¼ì†Œ: [ì£¼ì†Œ]
- {visit_month}ì›” {visit_region} ì§€ì—­ì˜ ì›”ë³„ ì—…ì¢…ë³„ ì´ìš©ê±´ìˆ˜ ìˆœìœ„ëŠ” [ì›”ë³„ ì—…ì¢…ë³„ ì´ìš©ê±´ìˆ˜ ìˆœìœ„]ìœ„ì˜€ìŠµë‹ˆë‹¤.
- ì›”ë³„ ì—…ì¢…ë³„ ì´ìš©ê¸ˆì•¡ ìˆœìœ„ëŠ” [ì›”ë³„ ì—…ì¢…ë³„ ì´ìš©ê¸ˆì•¡ ìˆœìœ„]ìœ„ì´ê³ , ê±´ë‹¹ í‰ê·  ì´ìš©ê¸ˆì•¡ ìˆœìœ„ëŠ” [ì›”ë³„ ì—…ì¢…ë³„ ê±´ë‹¹ í‰ê·  ì´ìš©ê¸ˆì•¡ ìˆœìœ„]ìœ„ì…ë‹ˆë‹¤.
- ì—°ë ¹ëŒ€ {user_age}ì˜ ë°©ë¬¸ ë¹„ìœ¨ì´ [ì—°ë ¹ëŒ€ë³„ ì´ìš©ë¹„ì¤‘]%ë¡œ {user_name}ê³¼ ë¹„ìŠ·í•œ ì—°ë ¹ëŒ€ì˜ ê³ ê°ì´ ë§ì´ ì°¾ì•˜ìŠµë‹ˆë‹¤.

ì£¼ë³€ ê´€ê´‘ì§€:
ë§›ì§‘ê³¼ ê°€ê¹Œìš´ ê³³ì— [ë§›ì§‘ ì£¼ë³€ ê´€ê´‘ì§€]ê°€ ìˆìŠµë‹ˆë‹¤.
ì¦ê±°ìš´ ì‹ì‚¬ì™€ ë©‹ì§„ ë°©ë¬¸ ë˜ì‹œê¸¸ ë°”ëë‹ˆë‹¤!"

Use the following pieces of retrieved context to answer the question.
[context]: {context}
---
[ì§ˆì˜]: {query}
---
[ë°ì´í„° ì„¤ëª…]
{user_age}: ì‚¬ìš©ìì˜ ì—°ë ¹ëŒ€,
{visit_month}: ì‚¬ìš©ìê°€ ì œì£¼ë„ë¥¼ ë°©ë¬¸í•˜ëŠ” ì›”,
{visit_times}: ì‚¬ìš©ìê°€ ë§›ì§‘ì„ ë°©ë¬¸í•  ì‹œê°„,
{visit_region}: ì‚¬ìš©ìê°€ ë°©ë¬¸í•˜ëŠ” ì œì£¼ë„ ì§€ì—­,
ì—…ì¢…-ìš”ì‹ê´€ë ¨ 30ê°œ ì—…ì¢…ìœ¼ë¡œ êµ¬ë¶„ (ì—…ì¢…ì´ 'ì»¤í”¼'ì¼ ê²½ìš° 'ì¹´í˜'ë¥¼ ëœ»í•¨)
"""
prompt = ChatPromptTemplate.from_template(template)


### 7. Google Gemini ëª¨ë¸ ìƒì„± ###
# @st.cache_resource
def load_model():
    system_instruction = """ë‹¹ì‹ ì€ ì œì£¼ë„ ì—¬í–‰ê°ì—ê²Œ ì œì£¼ë„ ë§›ì§‘ì„ ì¶”ì²œí•˜ëŠ” 'ì¹œì ˆí•œ ì œì£¼Â°C' ì±—ë´‡ì…ë‹ˆë‹¤.
        ì‚¬ìš©ìê°€ ëŒ€í™” ì¤‘ì— ì–¸ê¸‰í•œ ì—…ì¢…ì„ íŒŒì•…í•˜ê³  í•´ë‹¹ ì—…ì¢…ì˜ ë§›ì§‘ì„ ì¶”ì²œí•˜ì„¸ìš”.
        ì—°ë ¹ëŒ€ì— ë”°ë¼ ê´€ë ¨ ì¹¼ëŸ¼ì˜ ê°’ì„ ê³ ë ¤í•˜ê³ , ì‚¬ìš©ìê°€ ë°©ë¬¸í•˜ê³ ì í•˜ëŠ” ì‹œê°„ëŒ€ì˜ ì´ìš©ê±´ìˆ˜ ë¹„ì¤‘ì„ ì°¸ê³ í•˜ì—¬ ì¶”ì²œí•˜ì„¸ìš”.
        ì£¼ë³€ ê´€ê´‘ì§€ê°€ ìˆë‹¤ë©´ ì´ë¥¼ ì–¸ê¸‰í•˜ì—¬ ì‚¬ìš©ìì—ê²Œ ì¹œê·¼í•˜ê²Œ ê¶Œì¥í•˜ì„¸ìš”.
        ë©€í‹°í„´ ëŒ€í™”ë¥¼ ì§€ì›í•˜ë©° ì´ì „ ëŒ€í™”ì˜ ë§¥ë½ì„ í™œìš©í•˜ì—¬ í›„ì† ì§ˆë¬¸ì— ë‹µë³€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        Please ensure the response follows the provided format with clear sections and details.
        """
    model = ChatGoogleGenerativeAI(
        model="gemini-1.5-flash",
        temperature=0,
        max_tokens=5000,
        system_instruction=system_instruction
    )
    print("model loaded...")
    return model


### 8. ê²€ìƒ‰ ê²°ê³¼ í•„í„°ë§ & ë³‘í•© í•¨ìˆ˜ ###
# visit_region ë°ì´í„° í•„í„°ë§
def filter_results_by_region(docs, visit_region):
    return [doc for doc in docs if doc.metadata.get('ì§€ì—­') in visit_region]

def format_docs(docs):
  return "\n\n".join(doc.page_content for doc in docs)

def retrieve_and_filter_context(_input):
    # temp_retrieverì™€ retriever ê°ê° í˜¸ì¶œ ë° í•„í„°ë§ í›„ ë³‘í•©
    temp_docs = filter_results_by_region(temp_retriever.invoke(_input), visit_region)
    main_docs = filter_results_by_region(retriever.invoke(_input), visit_region)
    # ë³‘í•© í›„ í˜•ì‹í™”
    return format_docs(temp_docs + main_docs)

## 9. LangChain ì²´ì¸ êµ¬ì„± ###
rag_chain = (
  {"query":RunnablePassthrough(),
    "context": retrieve_and_filter_context,
    "user_name":RunnablePassthrough(),
    "user_age":RunnablePassthrough(),
    "visit_times":RunnablePassthrough(),
    "visit_month":RunnablePassthrough(),
    "visit_region":RunnablePassthrough()
  }
  # question(ì‚¬ìš©ìì˜ ì§ˆë¬¸) ê¸°ë°˜ìœ¼ë¡œ ì—°ê´€ì„±ì´ ë†’ì€ ë¬¸ì„œ retriever ìˆ˜í–‰ >> format_docsë¡œ ë¬¸ì„œë¥¼ í•˜ë‚˜ë¡œ ë§Œë“¦
  | prompt               # í•˜ë‚˜ë¡œ ë§Œë“  ë¬¸ì„œë¥¼ promptì— ë„˜ê²¨ì£¼ê³ 
  | load_model()         # llmì´ ì›í•˜ëŠ” ë‹µë³€ì„ ë§Œë“¦
  | StrOutputParser()
)


### 10. Streamlit UI ###
st.subheader("ğŸŠ:orange[ì œì£¼Â°C]ì—ê²Œ ì§ˆë¬¸í•˜ê¸°")
st.divider()

say_hi_to_user = """ì•ˆë…•í•˜ì„¸ìš”!  
ì œì£¼ë„ì˜ ì§€ì—­/ì‹œê°„ë³„ ê¸°ì˜¨ ë°ì´í„°ì— ê¸°ë°˜í•˜ì—¬ ë§›ì§‘ì„ ì¶”ì²œí•˜ëŠ” :orange[**ì¹œì ˆí•œ ì œì£¼Â°C**]ì…ë‹ˆë‹¤.  
ì–¸ì œë“ ì§€ ì§ˆë¬¸í•´ì£¼ì„¸ìš”."""

user_input = st.chat_input(
    placeholder="ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”. (ì˜ˆ: ì¶”ìë„ì— ìˆëŠ” ê°€ì •ì‹ ë§›ì§‘ì„ ì¶”ì²œí•´ì¤˜)",
    max_chars=150
)

chat_col1, search_col2 = st.columns([2, 1])
with search_col2:
    chat_search.show_search_restaurant()

    # ì±„íŒ… ê¸°ë¡ ì´ˆê¸°í™”
    if st.button("ì±„íŒ… ê¸°ë¡ ì´ˆê¸°í™”", type='primary'):
        st.session_state.messages = [
            {"role": "assistant", "content": say_hi_to_user}
        ]
        st.rerun()

with chat_col1:
    if 'messages' not in st.session_state:
        st.session_state.messages = [
            {"role": "assistant", "content": say_hi_to_user}
        ]
    # í•„ìˆ˜ ì •ë³´ê°€ ì…ë ¥ë˜ì§€ ì•Šì•˜ì„ ê²½ìš° ì˜¤ë¥˜ ë©”ì‹œì§€ ì¶œë ¥
    if not (user_age and visit_dates and visit_times and visit_region):
        st.error("ì‚¬ìš©ì ì •ë³´(ì—°ë ¹ëŒ€, ë°©ë¬¸ ë‚ ì§œ, ì‹œê°„, ì§€ì—­)ê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤. \nì™¼ìª½ ì‚¬ì´ë“œë°”ì—ì„œ ì •ë³´ë¥¼ ì…ë ¥í•´ ì£¼ì„¸ìš”.")
        st.stop()  # ì´í›„ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ì§€ ì•Šë„ë¡ ì¤‘ë‹¨


    for message in st.session_state.messages:
        avatar = "ğŸ§‘ğŸ»" if message['role'] == 'user' else botImgPath
        with st.chat_message(message['role'], avatar=avatar):
            st.markdown(message['content'])

    if user_input:
        st.session_state.messages.append({"role": "user", "content": user_input})
        with st.chat_message("user", avatar="ğŸ§‘ğŸ»"):
            st.markdown(user_input)

        # ì¶”ì²œ ìƒì„± ì¤‘ ìŠ¤í”¼ë„ˆ
        with st.spinner("ë§›ì§‘ ì°¾ëŠ” ì¤‘..."):
            query_text = user_input + f"""user_name: {user_name},
                                          user_age: {user_age},
                                          visit_region: {visit_region},
                                          visit_month: {visit_month},
                                          visit_times: {visit_times}"""
            
            # chain.invokeì—ì„œ ê°œë³„ ë³€ìˆ˜ë¡œ ì „ë‹¬
            assistant_response = rag_chain.invoke(query_text)

        # Assistant ì‘ë‹µ ê¸°ë¡ì— ì¶”ê°€ ë° ì¶œë ¥
        st.session_state.messages.append({"role": "assistant", "content": assistant_response})
        with st.chat_message("assistant", avatar=botImgPath):
            st.markdown(assistant_response)  
