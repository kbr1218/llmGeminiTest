# chat.py
import streamlit as st
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_community.vectorstores import Chroma
from langchain.embeddings import HuggingFaceEmbeddings
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.prompts import ChatPromptTemplate
from langchain_teddynote import logging
from dotenv import load_dotenv
import os

# í˜ì´ì§€ ì œëª© ì„¤ì •
st.set_page_config(page_title="main", page_icon="ğŸ’¬", layout="wide",
                   initial_sidebar_state='expanded')

from pages.subpages import sidebar, chat_search
from pages.subpages.modal import more

# CSS íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°
with open('style/chat_page.css', encoding='utf-8') as css_file:
    st.markdown(f"<style>{css_file.read()}</style>", unsafe_allow_html=True)

# ì‚¬ì´ë“œë°”
with st.sidebar:
    sidebar.show_sidebar()


########################################
# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')

# langsmith ì¶”ì  ì„¤ì •
logging.langsmith("bigcon_langchain_test")

# HuggingFace ì„ë² ë”© ìƒì„±
embeddings = HuggingFaceEmbeddings(model_name="jhgan/ko-sroberta-multitask")

### 1. Chroma ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ (í…ŒìŠ¤íŠ¸ìš© database_1000ì—ì„œ ë¶ˆëŸ¬ì˜´ ë‚˜ì¤‘ì— ìˆ˜ì • í•„ìš”) ###
vectorstore = Chroma(persist_directory="./database_1000", embedding_function=embeddings)

### 2. ì‚¬ìš©ì ì •ë³´ ê¸°ë°˜ ì§€ì—­ í•„í„°ë§ ###
user_name = st.session_state.get('user_name', [])
user_age = st.session_state.get('age', [])
visit_dates = st.session_state.get('visit_dates', [])
visit_times = st.session_state.get('visit_times', [])
visit_region = st.session_state.get('region', [])

# í•„í„° ì¡°ê±´ êµ¬ì„±
region_filter = {
    "area": {"$in": visit_region}
}

### 3. í•„í„°ë¥¼ ì ìš©í•˜ì—¬ ê²€ìƒ‰ê¸° ìƒì„± ###
retriever = vectorstore.as_retriever(search_type="mmr",
                                     search_kwargs={"k": 8,            # K: kê°œì˜ ë¬¸ì„œ ê²€ìƒ‰
                                                    "fetch_k": 10,
                                                    "filters":region_filter}) 


### 4. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„¤ì • (ìˆ˜ì • í•„ìš”: ë‚ ì”¨ì— ê¸°ë°˜í•˜ì—¬ ëŒ€ë‹µí•˜ë„ë¡ ìˆ˜ì •) ###
template = """
[context]: {context}
---
[ì§ˆì˜]: {query}
---
[ì˜ˆì‹œ]
ì„ íƒí•˜ì‹  ì œì£¼ë„ [ì„ íƒí•œ ì§€ì—­]ì— ìœ„ì¹˜í•œ ë§›ì§‘ì„ ì¶”ì²œí•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤!
[ì„ íƒí•œ ë°©ë¬¸ ì‹œê°„]ì— ë°©ë¬¸í•  ë§Œí•œ [ì•„ì¹¨ì‹ì‚¬] ë§›ì§‘ ì°¾ìœ¼ì‹œëŠ”êµ°ìš”.  
[visit_datesì˜ month]ì˜ ì˜¤ì „ì˜ í‰ê·  ê¸°ì˜¨ì€ ì•½ 00.0ë„ì…ë‹ˆë‹¤.
[ì‹ë‹¹ì´ë¦„]ì˜ [3ì›”] ì˜¤ì „(5ì‹œ-11ì‹œ) ë°©ë¬¸ìœ¨ì€ ì•½ 00.00%ë¡œ ë†’ì€ í¸ì…ë‹ˆë‹¤.

ì¶”ì²œ ì´ìœ :

ì¶”ê°€ ì •ë³´:
---
ë‹¹ì‹ ì€ ì£¼ì–´ì§„ [context]ì™€ í•„í„° ì¡°ê±´ì— ë§ê²Œ ì‘ë‹µí•´ì•¼ í•©ë‹ˆë‹¤.
í•„í„°ëœ ì§€ì—­ê³¼ ë¬¸ì„œì— ë”°ë¼ ë§ì¶¤í˜• ë§›ì§‘ì„ 3~5ê°œ ì¶”ì²œí•˜ê³ , ì´ìœ ë¥¼ ë°ì´í„° ê¸°ë°˜ìœ¼ë¡œ ì„¤ëª…í•˜ì„¸ìš”.
"""

# ìœ„ì˜ [context] ì •ë³´ ë‚´ì—ì„œ [ì§ˆì˜]ì— ëŒ€í•´ ë‹µë³€ [ì˜ˆì‹œ]ì™€ ê°™ì´ ìˆ ì–´ë¥¼ ë¶™ì—¬ì„œ ë‹µí•˜ì„¸ìš”.
# ì‚¬ìš©ìê°€ êµ¬ì²´ì ì¸ ìˆ«ìë¥¼ ì œì‹œí•˜ì§€ ì•Šì•˜ë‹¤ë©´, 3-5ê°œì˜ ë§›ì§‘ì„ ì¶”ì²œí•´ì£¼ì„¸ìš”.
# 'visit_region'ì€ area ë³€ìˆ˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì„ íƒë˜ì—ˆìŠµë‹ˆë‹¤. 
# ì¶”ì²œ ì´ìœ ëŠ” êµ¬ì²´ì ì¼ ìˆ˜ë¡ ì¢‹ìŠµë‹ˆë‹¤. ì™œ ì‚¬ìš©ìì—ê²Œ ì´ëŸ° ë§›ì§‘ì„ ì¶”ì²œí–ˆëŠ”ì§€ ë¹„ì¤‘ ë°ì´í„°ë¥¼ ê·¼ê±°ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”.

prompt = ChatPromptTemplate.from_template(template)


### 5. Google Gemini ëª¨ë¸ ìƒì„± ###
@st.cache_resource
def load_model():
    system_instruction = "ë‹¹ì‹ ì€ ì œì£¼ë„ ì—¬í–‰ê°ì—ê²Œ ì œì£¼ë„ ë§›ì§‘ì„ ì¶”ì²œí•˜ëŠ” ì¹œì ˆí•œ ì œì£¼ë„Â°C ì±—ë´‡ì…ë‹ˆë‹¤. ê±°ì§“ë§ì„ í•  ìˆ˜ ì—†ìœ¼ë©°, ì£¼ì–´ì§„ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì–˜ê¸°í•˜ì„¸ìš”."
    model = ChatGoogleGenerativeAI(model="gemini-1.5-flash",
                                   temperature=0.5,
                                   max_tokens=5000,
                                   system_instruction=system_instruction)
    print("model loaded...")
    return model
model = load_model()


### 6. ê²€ìƒ‰ ê²°ê³¼ ë³‘í•© í•¨ìˆ˜ ###
def merge_pages(pages):
    merged = "\n\n".join(page.page_content for page in pages)
    return merged


### 7. LangChain ì²´ì¸ êµ¬ì„± ###
chain = (
    {"query": RunnablePassthrough(),
     "context": retriever | merge_pages,    # retrieverë¡œ ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ merge_pages í•¨ìˆ˜ì— ì „ë‹¬
     "user_name":RunnablePassthrough(),     # RunnablePassThrough: ê°’ì„ ë³€ê²½í•˜ì§€ ì•Šê³  ê·¸ëŒ€ë¡œ í†µê³¼ì‹œí‚´
     "user_age":RunnablePassthrough(),
     "visit_times":RunnablePassthrough(),
     "visit_region": RunnablePassthrough(),
     "visit_dates": RunnablePassthrough(),
    }
    | prompt
    | load_model()
    | StrOutputParser()
)


### 8. streamlit UI ###
st.title("chat page")
st.divider()

user_input = st.chat_input(
    placeholder="ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”. (ì˜ˆ: ì¶”ìë„ì— ìˆëŠ” ë§›ì§‘ì„ ì•Œë ¤ì¤˜)",
    max_chars=150
)

chat_col1, search_col2 = st.columns([2, 1])

with chat_col1:
    # ëŒ€í™” ì´ë ¥ ì´ˆê¸°í™” ë° ì²« ë²ˆì§¸ ë©”ì‹œì§€
    if 'messages' not in st.session_state:
        st.session_state.messages = [
            {"role": "assistant", "content": """ì•ˆë…•í•˜ì„¸ìš”!  
            ì œì£¼ë„ì˜ ì§€ì—­/ì‹œê°„ë³„ ê¸°ì˜¨ ë°ì´í„°ì— ê¸°ë°˜í•˜ì—¬ ì¸ê¸°ìˆëŠ” ë§›ì§‘ì„ ì°¾ì•„ë“œë¦´ **ì¹œì ˆí•œ ì œì£¼ë„â„ƒ**ì…ë‹ˆë‹¤.  
            ê¶ê¸ˆí•œ ê²Œ ìˆë‹¤ë©´ ì–¸ì œë“  ì§ˆë¬¸í•´ì£¼ì„¸ìš”."""}
        ]

    # ì´ì „ ì±„íŒ… ê¸°ë¡ ì¶œë ¥
    for message in st.session_state.messages:
        avatar = "ğŸ˜Š" if message['role'] == 'user' else "ğŸŠ"
        with st.chat_message(message['role'], avatar=avatar):
            st.markdown(message['content'])

    # ì‚¬ìš©ì ì…ë ¥
    if user_input :
        st.session_state.messages.append({"role":"user", "content":user_input})
        with st.chat_message("user", avatar="ğŸ˜Š"):
            st.markdown(user_input)

        # ì¶”ì²œ ìƒì„± ì¤‘ ìŠ¤í”¼ë„ˆ
        with st.spinner("ë§›ì§‘ ì°¾ëŠ” ì¤‘..."):
            assistant_response = chain.invoke(user_input)

        # Assistant ì‘ë‹µ ê¸°ë¡ì— ì¶”ê°€ ë° ì¶œë ¥
        st.session_state.messages.append({"role": "assistant", "content": assistant_response})
        with st.chat_message("assistant", avatar="ğŸŠ"):
            st.markdown(assistant_response)

with search_col2:
    chat_search.show_search_restaurant()

    if st.button("ì§€ë„ë¡œ í™•ì¸í•˜ê¸°"):
        more.show_more_modal()


    #-----------------------------------------------------------

    # if user_input:
    #     # ì‚¬ìš©ì ì…ë ¥ì„ historyì— ì €ì¥
    #     st.session_state.chat_history.append({"role":"user", "content":user_input})
    #     with st.spinner("ì¶”ì²œì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤..."):
    #         # ì‹¤ì‹œê°„ spinnerë¡œ ì¤‘ê°„ ê²°ê³¼ ì¶œë ¥
    #         message_placeholder = st.empty()
    #         full_response = ""
    #         pages = retriever.get_relevant_documents(user_input)   # ê²€ìƒ‰ ê²°ê³¼
    #         context = merge_pages(pages)
    #         query = {"query":user_input, "context":context}

    #         # response_stream = chain.invoke_stream(query)    # ì‹¤ì‹œê°„ ì‘ë‹µ ë°›ê¸°

    #         # for chunk in response_stream:
    #         #     full_response += chunk.text
    #         #     message_placeholder.markdown(full_response)

    #         full_response = chain.invoke(query)
            
    #         # ìµœì¢… ì‘ë‹µ ì €ì¥
    #         st.session_state.chat_history.append({"role": "ai", "content": full_response})
    #         st.success("ì¶”ì²œ ê²°ê³¼:")
    #         st.write(full_response)
