# chat_sea.py
import os
import streamlit as st

from langchain_community.vectorstores import Chroma
from langchain.embeddings import HuggingFaceEmbeddings
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.prompts import ChatPromptTemplate
from dotenv import load_dotenv

# ì´ë¯¸ì§€ ë§í¬ ì„ ì–¸
botImgPath = 'https://raw.githubusercontent.com/kbr1218/streamlitTest/main/imgs/dolhareubang_sea.png'
seaImgPath = 'https://raw.githubusercontent.com/kbr1218/streamlitTest/main/imgs/sea_img.jpg'

# í˜ì´ì§€ ì œëª© ì„¤ì •
st.set_page_config(page_title="ì œì£¼ë„SEA", page_icon="ğŸ¬", layout="wide",
                   initial_sidebar_state='expanded')

from pages.subpages import sidebar
from pages.subpages import chat_search

# ì‚¬ì´ë“œë°”
with st.sidebar:
    sidebar.show_sidebar()


### 00. í™˜ê²½ë³€ìˆ˜ ë¡œë“œ ###
load_dotenv()
GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')


# ì„ë² ë”© ë° ë²¡í„°ìŠ¤í† ì–´ ì„¤ì •
EMBEDDING_MODEL_NAME = "jhgan/ko-sroberta-multitask"
VECTOR_DB_DIR = "./vector_database_sea"
embedding_model = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)
vectorstore = Chroma(persist_directory=VECTOR_DB_DIR, embedding_function=embedding_model)


# Google Gemini ëª¨ë¸ ì„¤ì •
def load_model():
    system_instruction = (
        "ë‹¹ì‹ ì€ ì œì£¼ë„ ì—¬í–‰ê°ì„ ìœ„í•œ ì¶”ì²œ ì±—ë´‡ì…ë‹ˆë‹¤. "
        "ì‚¬ìš©ì ì§ˆë¬¸ì— ì í•©í•œ ì •ë³´ë¥¼ ì œê³µí•˜ì„¸ìš”. ì œê³µëœ ë°ì´í„°ë§Œ í™œìš©í•˜ë©°, "
        "ì¶”ì¸¡ìœ¼ë¡œ ë‹µí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë°ì´í„°ê°€ ì¡´ì¬í•˜ëŠ”ë°, ì—†ë‹¤ê³  ë‹µí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
    )
    return ChatGoogleGenerativeAI(
        model="gemini-1.5-flash",
        temperature=0,
        max_tokens=5000,
        system_instruction=system_instruction,
        api_key=GOOGLE_API_KEY
    )

# ìƒˆë¡œìš´ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
prompt_template = """
íŠ¹ì • ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ 'ì œì£¼ë„ ë‚´ í•´ìˆ˜ìš•ì¥'ê³¼, 'í•´ë‹¹ í•´ìˆ˜ìš•ì¥ 1km ì´ë‚´ì˜ ê·¼ì²˜ ë§›ì§‘'ì„ ì¶”ì²œí•˜ëŠ” ì „ë¬¸ ì–´ì‹œìŠ¤í„´íŠ¸ 'ì¹œì ˆí•œ ì œì£¼ë„SEAğŸ–ï¸'ì…ë‹ˆë‹¤.
ì œê³µëœ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ì •í™•í•˜ê²Œ ë‹µë³€í•©ë‹ˆë‹¤. í™•ì‹¤í•˜ì§€ ì•Šì€ ê²½ìš° ëª¨ë¥¸ë‹¤ê³  ë‹µë³€í•©ë‹ˆë‹¤.
- ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ê¸°ì–µí•˜ê³  ë©€í‹°í„´ ë°©ì‹ìœ¼ë¡œ ë‹µë³€í•©ë‹ˆë‹¤.
- ì‚¬ìš©ìì˜ ì§ˆë¬¸ì´ ì¼ë°˜ì ì¸ ì¶”ì²œ ìš”ì²­ì¸ ê²½ìš° ì•„ë˜ì˜ êµ¬ì¡°í™”ëœ í˜•ì‹ì„ ë”°ë¥´ì„¸ìš”.
- ì‚¬ìš©ìì˜ ì§ˆë¬¸ì´ íŠ¹ì • ì •ë³´(ì˜ˆ: "í•´ë‹¹ ì›”ì— ê°€ì¥ ë”°ëœ»í•œ í•´ìˆ˜ìš•ì¥ì€ ì–´ë””ì¸ê°€ìš”?")ë¥¼ ìš”êµ¬í•˜ëŠ” ê²½ìš°, ì¶”ê°€ í˜•ì‹ì´ë‚˜ ì„¤ëª… ì—†ì´ ìš”ì²­ëœ ì •ë³´ë§Œ ì œê³µí•©ë‹ˆë‹¤.
- ì‚¬ìš©ìì˜ ì§ˆë¬¸ì´ í†µê³„ ë°ì´í„°ì— ê´€í•œ ê²ƒì¸ ê²½ìš°(ì˜ˆ: "í•´ë‹¹ ì›”ì— ê°€ì¥ ë”°ëœ»í•œ í•´ìˆ˜ìš•ì¥ì„ ë‚´ë¦¼ì°¨ìˆœìœ¼ë¡œ 5ê°œë§Œ ì•Œë ¤ì£¼ì„¸ìš”.") êµ¬ì²´ì ì¸ í†µê³„(í•„í„°ë§ í›„ ì •ë ¬) ê°’ì„ ì§ì ‘ì ì´ê³  ëª…í™•í•˜ê²Œ ì œê³µí•©ë‹ˆë‹¤.

í•­ìƒ ì£¼ì–´ì§„ ë°ì´í„° ì»¨í…ìŠ¤íŠ¸ì— ë”°ë¼ ë‹µë³€ì„ ì—„ê²©í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”.

ì¶”ì²œí•  ë•ŒëŠ” ì‚¬ìš©ìì—ê²Œ ì…ë ¥ ë°›ì€ ë‚ ì§œ ì •ë³´ì¤‘ì— 'ì›”(Month)', 'í‰ê· ìµœê³ ìˆ˜ì˜¨'/'í‰ê· ìµœì €ìˆ˜ì˜¨'ì„ ê³ ë ¤í•˜ì—¬ , 'í•´ìˆ˜ìš•ì¥(ì´ë¦„)' ê³¼ 'í•´ìˆ˜ìš•ì¥1kmê·¼ë°©ë§›ì§‘'ì„ ë¬¶ì–´ì„œ ìµœëŒ€ 1~3ê³³ì„ ì¶”ì²œí•˜ì„¸ìš”.

ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ì˜ ê´€ë ¨ í•´ìˆ˜ìš•ì¥ ì •ë³´ ìš”ì•½ìœ¼ë¡œ ì‹œì‘í•˜ì—¬ {visit_month}ì— ëŒ€í•´ í•´ìˆ˜ìš•ì¥ ë° ë ˆìŠ¤í† ë‘ ì¶”ì²œì„ ê³„ì†í•´ì•¼ í•©ë‹ˆë‹¤.

ë‹¤ìŒ ì—´ì€ ìµœìƒì˜ ê¶Œì¥ ì‚¬í•­ì„ ì°¾ëŠ” ë° ê´€ë ¨ì´ ìˆìŠµë‹ˆë‹¤:
- ['ì›”'] ì¹¼ëŸ¼: {{ì›”}}
- ['í‰ê· ìµœê³ ìˆ˜ì˜¨'] ì¹¼ëŸ¼: {{í‰ê· ìµœê³ ìˆ˜ì˜¨}}
- ['í‰ê· ìµœì €ìˆ˜ì˜¨'] ì¹¼ëŸ¼: {{í‰ê· ìµœì €ìˆ˜ì˜¨}}

ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ì™€ ì‚¬ìš©ì ì •ë³´ë¥¼ ì—„ê²©í•˜ê²Œ ì‚¬ìš©í•˜ì„¸ìš”:
[context]: {context}
[previous_chat_history]: {previous_chat_history}
---
[ì§ˆì˜]: {query}
"""
prompt = ChatPromptTemplate.from_template(prompt_template)


# LangChain ì²´ì¸ êµ¬ì„±
rag_chain = (
    {
        "query": RunnablePassthrough(),
        "context": lambda q: vectorstore.similarity_search(q["query"], k=22),
        "visit_month": RunnablePassthrough(),
        "recommendations": RunnablePassthrough(),  # Ensure recommendations is passed
        "previous_chat_history": RunnablePassthrough()  # ì¶”ê°€ëœ í•„ë“œ ì „ë‹¬
    }
    | prompt
    | load_model()
    | StrOutputParser()
)


# Streamlit ìƒíƒœ ì´ˆê¸°í™”
if "conversation" not in st.session_state:
    st.session_state["conversation"] = []
if "user_name" not in st.session_state:
    st.session_state["user_name"] = None
if "age" not in st.session_state:
    st.session_state["age"] = None
if "visit_dates" not in st.session_state:
    st.session_state["visit_dates"] = None
if "visit_times" not in st.session_state:
    st.session_state["visit_times"] = None
if "region" not in st.session_state:
    st.session_state["region"] = []
if "selected_option" not in st.session_state:
    st.session_state["selected_option"] = "ì œì£¼ë„SEA ì±—ë´‡ê³¼ ë°”ë¡œ ëŒ€í™”í•˜ê¸°"
if "context" not in st.session_state:
    st.session_state["context"] = ""
if "last_recommended_beach" not in st.session_state:
    st.session_state["last_recommended_beach"] = None

# ë°©ë¬¸ ì›” ê³„ì‚° (visit_month)
visit_dates = st.session_state.get("visit_dates")
visit_month = visit_dates.month if visit_dates else None



### 10. Streamlit UI ###
st.subheader("ğŸ¬:blue[ì œì£¼ë„ SEA]ì—ê²Œ ì§ˆë¬¸í•˜ê¸°")
st.caption("ğŸš€ 2024 ë¹…ì½˜í…ŒìŠ¤íŠ¸ (ìƒì„±í˜• AI ë¶„ì•¼) íŒ€: í—¬ë¡œë¹…ì½˜")
st.divider()

st.markdown(
    """
    ì•ˆë…•í•˜ì„¸ìš”ğŸ˜ ì œì£¼ë„ í•´ìˆ˜ìš•ì¥ ì¶”ì²œ ì±—ë´‡ ğŸ¬:blue[**ì œì£¼ë„ SEA**]ì…ë‹ˆë‹¤ :)  
    ì œì£¼ë„ ë°”ë‹¤ ìˆ˜ì˜¨ì„ ê¸°ë°˜ìœ¼ë¡œ ìˆ˜ì˜í•˜ê¸° ì¢‹ì€ **í•´ìˆ˜ìš•ì¥**ğŸ–ï¸ê³¼ **ë¬¼ë†€ì´ ë³µì¥**ğŸ©±ì„ ì¶”ì²œí•˜ê³ ,  
    ì¶”ì²œëœ í•´ìˆ˜ìš•ì¥ ë°˜ê²½ 1km ë‚´ ë§›ì§‘ì„ ì¶”ì²œí•´ë“œë¦½ë‹ˆë‹¤ğŸŠ  
    (ë§›ì§‘ ë°ì´í„°: ì‹ í•œì¹´ë“œ ì œì£¼ ê°€ë§¹ì  ì´ìš© ë°ì´í„°)
    """
)

# ë°”ë‹¤ ì´ë¯¸ì§€
seaImg = (f"""
<div>
    <img src="{seaImgPath}" alt="sea image" width=100%>
</div>
""")
st.markdown(seaImg, unsafe_allow_html=True)

say_hi_to_user_sea = """ğŸ¬ ì œì£¼ë„ í•´ìˆ˜ìš•ì¥ì— ëŒ€í•´ ê¶ê¸ˆí•œ ì ì„ ë¬¼ì–´ë³´ì„¸ìš”.  
ì…ë ¥í•˜ì‹  ì›” ì •ë³´ë¥¼ í† ëŒ€ë¡œ í•´ìˆ˜ìš•ì¥ì„ ì¶”ì²œë“œë¦¬ê³  ìˆì–´ìš” :)  
"""

chat_input = st.chat_input(
    placeholder="ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”. (ì˜ˆ: ìš°ë„ì— ìˆëŠ” í•´ìˆ˜ìš•ì¥ì„ ì¶”ì²œí•´ì¤˜)",
    max_chars=150,
)

if 'messages_sea' not in st.session_state:
    st.session_state["messages_sea"] = [
        {"role": "assistant", "content": say_hi_to_user_sea}
    ]

for message in st.session_state["messages_sea"]:
    role = "user" if message["role"] == "user" else "assistant"
    avatar = "ğŸ§‘ğŸ»" if role == "user" else botImgPath
    if role == "assistant":
        with st.chat_message(message['role'], avatar=botImgPath):
            st.markdown(message["content"])
    else:
        with st.chat_message(role, avatar=avatar):
            st.markdown(message["content"])

if chat_input:
    st.session_state["messages_sea"].append({"role": "user", "content": chat_input})
    with st.chat_message("user", avatar="ğŸ§‘ğŸ»"):
        st.markdown(chat_input)

    # ì´ì „ ëŒ€í™” ë‚´ìš©ì„ í¬í•¨í•˜ì—¬ ì¶”ì²œ ì •ë³´ ìƒì„±
    previous_chat_history = "\n".join(
        [f"{msg['role']}: {msg['content']}" for msg in st.session_state.get("messages_sea", [])]
    )

    with st.spinner("ì¶”ì²œ ì •ë³´ë¥¼ ìƒì„± ì¤‘..."):
        response = rag_chain.invoke({
            "query": chat_input,
            "visit_month": visit_month,
            "context": st.session_state["context"],
            "recommendations": "",  # ê¸°ë³¸ ê°’ ì„¤ì •
            "previous_chat_history": previous_chat_history,  # ì¶”ê°€ëœ í•„ë“œ
        })

        st.session_state["messages_sea"].append({"role": "assistant", "content": response})
        with st.chat_message("assistant", avatar=botImgPath):
            st.markdown(response)
