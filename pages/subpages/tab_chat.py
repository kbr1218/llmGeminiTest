# tab_chat.py
import streamlit as st
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_community.vectorstores import Chroma
from langchain.embeddings import HuggingFaceEmbeddings
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.prompts import ChatPromptTemplate
from langchain_teddynote import logging
from dotenv import load_dotenv
import os

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')

# langsmith ì¶”ì  ì„¤ì •
logging.langsmith("bigcon_langchain_test")

# HuggingFace ì„ë² ë”© ìƒì„±
embeddings = HuggingFaceEmbeddings(model_name="jhgan/ko-sroberta-multitask")

### 1. Chroma ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ (í…ŒìŠ¤íŠ¸ìš© database_1000ì—ì„œ ë¶ˆëŸ¬ì˜´ ë‚˜ì¤‘ì— ìˆ˜ì • í•„ìš”) ###
vectorstore = Chroma(persist_directory="./database_1000", embedding_function=embeddings)

### 2. ê²€ìƒ‰ê¸° ìƒì„± ###
retriever = vectorstore.as_retriever(search_type="mmr",
                                     search_kwargs={"k": 8, "fetch_k": 10})  # K: kê°œì˜ ë¬¸ì„œ ê²€ìƒ‰



### 3. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„¤ì • (ìˆ˜ì • í•„ìš”: ë‚ ì”¨ì— ê¸°ë°˜í•˜ì—¬ ëŒ€ë‹µí•˜ë„ë¡ ìˆ˜ì •) ###
template = """
[context]: {context}
---
[ì§ˆì˜]: {query}
---
[ì˜ˆì‹œ]
ì œì£¼ë„ì— ìœ„ì¹˜í•œ ë§›ì§‘ì…ë‹ˆë‹¤
**ê°€ê²Œëª…**: ì§€ì—­ êµ¬ë¶„, ì¶”ì²œ ì´ìœ 
---
ì œì£¼ë„ ë‚´ í•«í”Œë ˆì´ìŠ¤ ë§›ì§‘ì„ ì¶”ì²œí•˜ëŠ” ëŒ€í™”í˜• AI assistant ì—­í• ì„ í•´ì£¼ì„¸ìš”.
ì£¼ì–´ì§„ ë°ì´í„°ëŠ” ì‹ í•œì¹´ë“œì— ë“±ë¡ëœ ê°€ë§¹ì  ì¤‘ ë§¤ì¶œ ìƒìœ„ 9,252ê°œ ìš”ì‹ì—…ì¢…(ìŒì‹ì , ì¹´í˜ ë“±)ì…ë‹ˆë‹¤.
ë°ì´í„°ì— ëŒ€í•œ ë©”íƒ€ ë°ì´í„°ëŠ” ì²«ë²ˆì§¸ì™€ ë‘ë²ˆì§¸ í–‰ì— ìˆìŠµë‹ˆë‹¤.

ìœ„ì˜ [context] ì •ë³´ ë‚´ì—ì„œ [ì§ˆì˜]ì— ëŒ€í•´ ë‹µë³€ [ì˜ˆì‹œ]ì™€ ê°™ì´ ìˆ ì–´ë¥¼ ë¶™ì—¬ì„œ ë‹µí•˜ì„¸ìš”.
ì‚¬ìš©ìê°€ êµ¬ì²´ì ì¸ ìˆ«ìë¥¼ ì œì‹œí•˜ì§€ ì•Šì•˜ë‹¤ë©´, 3-5ê°œì˜ ë§›ì§‘ì„ ì¶”ì²œí•´ì£¼ì„¸ìš”.
'ì œì£¼ë„ ì§€ì—­ êµ¬ë¶„'ì€ area ë³€ìˆ˜ë¥¼ ì°¸ê³ í•´ì„œ ë‹µë³€í•´ì£¼ì„¸ìš”. 
ì¶”ì²œ ì´ìœ ëŠ” êµ¬ì²´ì ì¼ ìˆ˜ë¡ ì¢‹ìŠµë‹ˆë‹¤. ì™œ ì‚¬ìš©ìì—ê²Œ ì´ëŸ° ë§›ì§‘ì„ ì¶”ì²œí–ˆëŠ”ì§€ ë¹„ì¤‘ ë°ì´í„°ë¥¼ ê·¼ê±°ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”.
"""
prompt = ChatPromptTemplate.from_template(template)



### 4. Google Gemini ëª¨ë¸ ìƒì„± ###
@st.cache_resource
def load_model():
    # ChatGoogleGenerativeAI.configure(api_key=GOOGLE_API_KEY)
    system_instruction = "ë‹¹ì‹ ì€ ì œì£¼ë„ ì—¬í–‰ê°ì—ê²Œ ì œì£¼ë„ ë§›ì§‘ì„ ì¶”ì²œí•˜ëŠ” ì¹œì ˆí•œ ì œì£¼ë„Â°C ì±—ë´‡ì…ë‹ˆë‹¤. ê±°ì§“ë§ì„ í•  ìˆ˜ ì—†ìœ¼ë©°, ì£¼ì–´ì§„ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì–˜ê¸°í•˜ì„¸ìš”."
    model = ChatGoogleGenerativeAI(model="gemini-1.5-flash",
                                   temperature=0.5,
                                   max_tokens=5000,
                                   system_instruction=system_instruction)
    
    print("model loaded...")
    return model
model = load_model()


### 5. ê²€ìƒ‰ ê²°ê³¼ ë³‘í•© í•¨ìˆ˜ ###
def merge_pages(pages):
    merged = "\n\n".join(page.page_content for page in pages)
    return merged


### 6. LangChain ì²´ì¸ êµ¬ì„± ###
chain = (
    {"query": RunnablePassthrough(), "context": retriever | merge_pages}
    | prompt
    | load_model()
    | StrOutputParser()
)



### 7. streamlit UI ###
def show_tab_chat():
    st.subheader("gemini chatbot here")

    # ëŒ€í™” ì´ë ¥ ì´ˆê¸°í™”
    if 'messages' not in st.session_state:
        st.session_state.messages = []

    # ì´ì „ ì±„íŒ… ê¸°ë¡ ì¶œë ¥
    for message in st.session_state.messages:
        with st.chat_message(message['role']):
            st.markdown(message['content'])

    # ì‚¬ìš©ì ì…ë ¥
    if user_input := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”. (ì˜ˆ: ì¶”ìë„ ë§›ì§‘ì„ ì¶”ì²œí•´ì¤˜)"):
        st.session_state.messages.append({"role":"user", "content":user_input})
        with st.chat_message("user", avatar="ğŸ˜Š"):
            st.markdown(user_input)
        
        # ì¶”ì²œ ìƒì„± ì¤‘ ìŠ¤í”¼ë„ˆ
        with st.spinner("ë§›ì§‘ ì°¾ëŠ” ì¤‘..."):
            assistant_response = chain.invoke(user_input)

        # Assistant ì‘ë‹µ ê¸°ë¡ì— ì¶”ê°€ ë° ì¶œë ¥
        st.session_state.messages.append({"role": "assistant", "content": assistant_response})
        with st.chat_message("assistant", avatar="ğŸŠ"):
            st.markdown(assistant_response)


    #-----------------------------------------------------------

    # # ì‚¬ìš©ì ì…ë ¥ì°½
    # user_input = st.text_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”",
    #                            placeholder="ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”. (ì˜ˆ: ì¶”ìë„ ë§›ì§‘ì„ ì¶”ì²œí•´ì¤˜)",  # ì˜ˆì‹œë„ ìˆ˜ì • í•„ìš”
    #                            label_visibility='collapsed')

    # if user_input:
    #     # ì‚¬ìš©ì ì…ë ¥ì„ historyì— ì €ì¥
    #     st.session_state.chat_history.append({"role":"user", "content":user_input})
    #     with st.spinner("ì¶”ì²œì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤..."):
    #         # ì‹¤ì‹œê°„ spinnerë¡œ ì¤‘ê°„ ê²°ê³¼ ì¶œë ¥
    #         message_placeholder = st.empty()
    #         full_response = ""
    #         pages = retriever.get_relevant_documents(user_input)   # ê²€ìƒ‰ ê²°ê³¼
    #         context = merge_pages(pages)
    #         query = {"query":user_input, "context":context}

    #         # response_stream = chain.invoke_stream(query)    # ì‹¤ì‹œê°„ ì‘ë‹µ ë°›ê¸°

    #         # for chunk in response_stream:
    #         #     full_response += chunk.text
    #         #     message_placeholder.markdown(full_response)


    #         full_response = chain.invoke(query)
            
    #         # ìµœì¢… ì‘ë‹µ ì €ì¥
    #         st.session_state.chat_history.append({"role": "ai", "content": full_response})
    #         st.success("ì¶”ì²œ ê²°ê³¼:")
    #         st.write(full_response)

    #         # answer = chain.invoke(user_input)
    #         # st.success("ì¶”ì²œ ê²°ê³¼:")
    #         # st.write(answer)

    # # ëŒ€í™” ì´ë ¥ ì¶œë ¥
    # for message in st.session_state.chat_history:
    #     role = "user" if message["role"] == "user" else "ai"
    #     with st.chat_message(role):
    #         st.markdown(message["content"])